\documentclass[11pt,a4paper]{article}

% ============================================================
%  Packages
% ============================================================
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{natbib}
\usepackage{microtype}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{listings}

% ============================================================
%  Hyperref
% ============================================================
\hypersetup{
  colorlinks=true,
  linkcolor=blue!70!black,
  citecolor=green!50!black,
  urlcolor=blue!70!black
}

% ============================================================
%  Header / Footer
% ============================================================
\pagestyle{fancy}
\fancyhf{}
\rhead{\small Amazon Fine Food Review --- Sentiment Analysis}
\lhead{\small NLP Project Report}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ============================================================
%  Section formatting
% ============================================================
\titleformat{\section}{\large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection.}{0.5em}{}
\titleformat{\subsubsection}{\normalsize\itshape}{\thesubsubsection.}{0.5em}{}

% ============================================================
%  Custom
% ============================================================
\definecolor{bestcolor}{RGB}{0,120,0}
\definecolor{warncolor}{RGB}{180,50,0}
\newcommand{\best}[1]{\textcolor{bestcolor}{\textbf{#1}}}
\newcommand{\warn}[1]{\textcolor{warncolor}{\textit{#1}}}

% ============================================================
\begin{document}
% ============================================================

% ---- Title Page ----
\begin{titlepage}
  \centering
  \vspace*{1.8cm}

  {\LARGE\bfseries Sentiment Analysis of Amazon Fine Food Reviews:\\[0.4em]
   A Comparative Study of Classical NLP Classifiers\\[0.4em]
   under Class Imbalance\par}

  \vspace{1.5cm}

  {\large Ricky Gong\par}
  \vspace{0.2cm}
  {\normalsize University of Pennsylvania\par}
  \vspace{0.1cm}
  {\normalsize \href{mailto:gong8@seas.upenn.edu}{gong8@seas.upenn.edu}\par}
  \vspace{0.4cm}
  {\normalsize \today\par}

  \vspace{2cm}

  \begin{abstract}
  \noindent
  This paper presents a rigorous, baseline-driven NLP pipeline for binary
  sentiment classification of the Amazon Fine Food Reviews corpus
  (568,454 records spanning 13 years).
  %
  Given only the raw review text, our system predicts whether the expressed
  opinion is \textit{positive} or \textit{negative}.
  %
  We address three core challenges:
  (1)~high-dimensional sparse text representations,
  (2)~significant class imbalance (78\% positive, 22\% negative), and
  (3)~a business requirement to maximise recall for the minority negative class
  without collapsing precision to unacceptable levels.
  %
  Our methodology proceeds from a carefully designed \emph{baseline-first}
  strategy: we establish a Logistic Regression baseline on raw TF-IDF
  bi-gram features, then systematically evaluate class weighting, SMOTE,
  and random undersampling, followed by ElasticNet regularisation and
  threshold tuning.
  %
  A key corrective finding is that standard stop-word removal
  \emph{harms} sentiment classification because it discards negation words
  (``not'', ``won't'', ``can't'') that are semantically critical.
  %
  We also demonstrate empirically that Truncated SVD is an inappropriate
  primary dimensionality-reduction strategy for TF-IDF NLP features:
  1,000 components explain less than 70\% of the total variance, far below
  the threshold that would justify information loss.
  %
  Our best single model --- a balanced, ElasticNet-regularised Logistic
  Regression with threshold tuning ($\tau^*=0.37$) --- achieves a
  negative-class F$_1$ of \best{0.784}, precision of \best{0.750},
  recall of \best{0.821}, and AUC of \best{0.951}.
  %
  A Soft Voting ensemble (Logistic Regression + Random Forest) achieves
  the same AUC of \best{0.951} with complementary precision--recall balance,
  demonstrating that model complementarity can be exploited without retraining.
  \end{abstract}

  \vspace{1cm}

  {\small\textbf{Keywords:}
   sentiment analysis, NLP, TF-IDF, class imbalance,
   logistic regression, random forest, ensemble learning,
   stop-word handling, threshold tuning, Amazon reviews}
\end{titlepage}

\tableofcontents
\newpage

% ============================================================
\section{Introduction}
% ============================================================

Online product reviews have become a primary channel for customer feedback.
The Amazon Fine Food Reviews dataset, with over half a million reviews, is a
canonical NLP benchmark that captures the full spectrum of food product
opinions from 1999 to 2012.
Automated sentiment analysis of such corpora enables businesses to monitor
product quality, identify dissatisfied customers early, and improve
recommendation systems \citep{pang2008opinion}.

This paper makes four main contributions:

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Corrected stop-word handling for sentiment analysis.}
        We show that removing standard stop words --- which include
        negation words such as \textit{not}, \textit{won't}, and
        \textit{can't} --- degrades classifier performance by inverting
        the polarity of negated phrases.
        We propose a custom stop-word list that preserves negation.

  \item \textbf{Empirical critique of SVD for NLP dimensionality reduction.}
        We demonstrate that Truncated SVD on TF-IDF features retains only
        30\% of the variance at 300 components and $\approx$55\% at 1,000,
        making it unsuitable as a primary feature-extraction step.
        We reframe SVD as a \emph{necessary pre-processing step} for
        algorithms requiring dense input (e.g., SMOTE), not a general-purpose
        improvement.

  \item \textbf{Threshold tuning as a zero-cost deployment strategy.}
        After model training, adjusting the classification threshold allows
        deployment teams to dynamically balance precision and recall without
        retraining. We provide a full precision--recall curve analysis.

  \item \textbf{A systematic model comparison.}
        Eight model configurations are evaluated under consistent conditions,
        providing a clear picture of the marginal benefit of each technique.
\end{enumerate}

% ============================================================
\section{Related Work}
\label{sec:related}
% ============================================================

Sentiment analysis has progressed from lexicon-based methods
\citep{hutto2014vader,baccianella2010sentiwordnet} to machine learning
\citep{pang2002thumbs} and deep learning approaches \citep{maas2011learning}.

\citet{pang2002thumbs} is the seminal ML work on sentiment classification,
showing that Support Vector Machines and Naive Bayes outperform human-crafted
rules on movie reviews.
They also observe that stop-word removal is harmful for sentiment tasks ---
a finding we independently replicate here.
\citet{mcauley2013hidden} specifically study Amazon review data, using
latent factor models to jointly capture user preferences and sentiment.

More recently, Transformer-based models such as BERT
\citep{devlin2019bert} achieve near-human performance on fine-grained
sentiment tasks.
However, classical ML models remain competitive on well-curated binary
classification tasks, offer superior interpretability via feature importance
coefficients, and are far more computationally accessible
\citep{pang2008opinion}.
This paper deliberately focuses on classical ML for these practical reasons.

% ============================================================
\section{Dataset and Exploratory Analysis}
\label{sec:data}
% ============================================================

\subsection{Dataset Description}

The Amazon Fine Food Reviews dataset\footnote{%
  \url{https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews}}
contains 568,454 reviews across 10 fields: review id, product id,
user id, profile name, helpfulness votes, score (1--5), timestamp,
summary, and review text.

\subsection{Label Construction and Class Imbalance}

We map the 5-point Likert score to binary sentiment:

\begin{equation}
  y_i = \begin{cases}
    1 \;(\text{positive}) & \text{if } s_i > 3 \\
    0 \;(\text{negative}) & \text{if } s_i < 3 \\
    \text{excluded}       & \text{if } s_i = 3
  \end{cases}
\end{equation}

Reviews with score~3 represent ambiguous opinions and are excluded to reduce
label noise.
After exclusion, the class distribution is
\textbf{443,777 positive (78.1\%)} and \textbf{124,677 negative (21.9\%)}.
This moderate but non-trivial imbalance motivates the strategies in
Section~\ref{sec:imbalance}.

\subsection{Exploratory Observations}

Score distributions are sharply bimodal: 5-star reviews account for 52\% of
all records; 1-star reviews account for 10\%.
Review length (in words) follows a heavy-tailed distribution with a median of
approximately 55 words.
Word clouds of preprocessed summaries confirm strong lexical separation
between classes: positive reviews are dominated by \textit{great},
\textit{love}, \textit{best}, and \textit{perfect}; negative reviews by
\textit{disappoint}, \textit{terrible}, \textit{return}, and \textit{worst}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_eda.pdf}
  \caption{Raw star-rating distribution (left) and binary sentiment label
           distribution (right) after excluding neutral scores (score~=~3).
           The dataset is imbalanced: 78.1\% positive, 21.9\% negative.}
  \label{fig:eda}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_wordcloud.pdf}
  \caption{Word clouds of preprocessed review text for positive (left) and
           negative (right) classes. Prominent terms confirm strong lexical
           separation between polarities.}
  \label{fig:wordcloud}
\end{figure}

% ============================================================
\section{Methodology}
\label{sec:method}
% ============================================================

\subsection{Text Preprocessing}

\subsubsection{Stop-Word Handling}

A critical design decision in sentiment NLP is whether to apply
stop-word removal.
Standard stop-word lists (e.g., NLTK's English list) include negation words
such as:

\begin{center}
\textit{not, no, nor, never, neither, nobody, nothing, nowhere,\\
don't, doesn't, didn't, won't, wouldn't, can't, couldn't, shouldn't,\\
isn't, aren't, wasn't, weren't, haven't, hasn't, hadn't, ain't}
\end{center}

Removing these words corrupts the sentiment signal:

\begin{table}[H]
  \centering
  \caption{Effect of stop-word removal on sentiment-carrying phrases.}
  \label{tab:stopwords}
  \begin{tabular}{lll}
    \toprule
    \textbf{Original phrase} & \textbf{After removal} & \textbf{Signal preserved?} \\
    \midrule
    ``not great''        & ``great''      & \textcolor{red}{No --- inverted} \\
    ``won't buy again''  & ``buy''        & \textcolor{red}{No --- lost negative} \\
    ``can't recommend''  & ``recommend''  & \textcolor{red}{No --- inverted} \\
    ``absolutely love''  & ``absolutely love'' & \textcolor{green!60!black}{Yes} \\
    \bottomrule
  \end{tabular}
\end{table}

We construct a \textbf{custom stop-word list} that removes the 13
negation/contraction words from the standard 179-word NLTK list,
retaining them as features.
This is consistent with the recommendation in \citet{pang2002thumbs}.

\subsubsection{Cleaning Pipeline}

Text undergoes the following steps in order:

\begin{enumerate}[leftmargin=2em]
  \item Lowercasing.
  \item Punctuation removal (punctuation replaced by whitespace;
        apostrophes preserved to maintain contractions such as
        \textit{won't}, \textit{can't}).
  \item Tokenisation (whitespace splitting).
  \item Custom stop-word filtering (retaining negation words).
  \item Snowball stemming: \textit{tasty}~$\to$~\textit{tasti},
        \textit{disappointment}~$\to$~\textit{disappoint}.
\end{enumerate}

\subsection{Feature Engineering: TF-IDF with Bi-grams}
\label{sec:features}

\subsubsection{TF-IDF Formulation}

For term $t$, document $d$, and corpus $D$:

\begin{equation}
  \text{TF}(t,d) = \frac{\mathrm{count}(t,d)}{\sum_{t'}\mathrm{count}(t',d)},
  \quad
  \text{IDF}(t,D) = \log\!\left(\frac{|D|+1}{|\{d' \in D : t \in d'\}|+1}\right)+1
\end{equation}

\begin{equation}
  \text{TF-IDF}(t,d,D) = \text{TF}(t,d) \times \text{IDF}(t,D)
\end{equation}

We apply \texttt{sublinear\_tf=True}, replacing raw TF with $\log(1 + \mathrm{TF})$
to dampen the effect of extremely high-frequency terms.

\subsubsection{N-gram Range}

We use \textbf{unigrams + bigrams} (\texttt{ngram\_range=(1,2)}).
Bigrams preserve negation context that unigrams lose:
``not great'' as a bigram has opposite polarity to the unigram ``great''.
The vocabulary is capped at \texttt{max\_features=10,000} to control memory.
The resulting TF-IDF matrix has shape $(568{,}454,\;10{,}000)$
with approximately $0.45\%$ density (highly sparse).

\subsection{Train/Test Split}

The dataset is split 80\%/20\% with stratified sampling (\texttt{random\_state=42}),
yielding 454,763 training and 113,691 test records.
All preprocessing fitted to data (SVD parameters, SMOTE, cross-validation)
is applied exclusively to the training partition to prevent data leakage.

\subsection{Baseline-First Development Principle}

Before applying any complexity (class balancing, regularisation,
dimensionality reduction), we train a vanilla Logistic Regression on
raw TF-IDF features.
This baseline serves as the reference point for measuring
the marginal benefit of each subsequent technique.
Skipping this step makes it impossible to determine whether a particular
modification actually improves performance.

\subsection{Logistic Regression}

For binary target $y \in \{0,1\}$, Logistic Regression models:

\begin{equation}
  P(y=1 \mid \mathbf{x}) = \sigma(\mathbf{w}^\top\mathbf{x} + b)
  = \frac{1}{1+e^{-(\mathbf{w}^\top\mathbf{x}+b)}}
\end{equation}

Parameters are estimated by minimising cross-entropy loss with optional
ElasticNet regularisation:

\begin{equation}
  \mathcal{L}(\mathbf{w}) = -\frac{1}{N}\sum_{i=1}^{N}
  \left[y_i\log\hat{p}_i + (1-y_i)\log(1-\hat{p}_i)\right]
  + \lambda\!\left[\alpha\|\mathbf{w}\|_1 + (1-\alpha)\|\mathbf{w}\|_2^2\right]
  \label{eq:en}
\end{equation}

The mixing parameter $\alpha$ (\texttt{l1\_ratio}) controls the
L1/L2 balance: $\alpha=0$ is pure Ridge, $\alpha=1$ is pure Lasso.

\subsection{Handling Class Imbalance}
\label{sec:imbalance}

We evaluate three strategies:

\paragraph{Class Weights.}
The loss is re-weighted inversely proportional to class frequency:

\begin{equation}
  w_c = \frac{N}{K \cdot N_c}
\end{equation}

where $N$ = total samples, $K$ = number of classes, $N_c$ = samples in
class $c$. This is the most computationally efficient strategy --- no data
modification is needed.

\paragraph{SMOTE.}
Synthetic minority samples are created by interpolation in feature space
\citep{chawla2002smote}:

\begin{equation}
  \mathbf{x}_{\mathrm{new}} = \mathbf{x}_i
  + \lambda(\mathbf{x}_{\mathrm{nn}} - \mathbf{x}_i),
  \quad \lambda \sim \mathrm{Uniform}(0,1)
\end{equation}

SMOTE requires \textbf{dense} input.
Because TF-IDF produces a sparse matrix, we first apply Truncated SVD
to obtain a dense representation.
This introduces SVD as a \emph{necessary preprocessing step} for SMOTE
--- not as a standalone feature-engineering improvement.

\paragraph{Random Undersampling.}
Majority-class samples are randomly discarded until class balance is achieved.
Fast and effective, but potentially discards useful information.

\subsection{Critical Analysis of Truncated SVD for NLP}
\label{sec:svd}

Truncated SVD (Latent Semantic Analysis) decomposes the TF-IDF matrix:

\begin{equation}
  \mathbf{X} \approx \mathbf{U}_{:,1:k}\,\boldsymbol{\Sigma}_{1:k,1:k}\,\mathbf{V}_{1:k,:}^\top
\end{equation}

In many domains (e.g., tabular data, image features), the top 3--10
principal components explain $\geq 90\%$ of the variance.
For NLP TF-IDF features, however, the variance is distributed across
many dimensions due to the high sparsity and diverse vocabulary.
Our experiments confirm that \textbf{300 SVD components explain only
$\approx$30\% of the total variance}, and 1,000 components capture
approximately 55\% --- far below the threshold that would justify the
information loss.
Figure~\ref{fig:svd} shows the cumulative and marginal explained variance:
the curve does not reach 90\% within 1,000 components,
with rapidly diminishing marginal returns visible from component 50 onward.

\textbf{Recommendation}: Do not use SVD as a primary dimensionality-reduction
strategy for NLP tasks. Use it only when dense input is required by a
downstream algorithm. Future work should explore dense representations
such as Word2Vec average pooling or fine-tuned Transformer embeddings
(e.g., DistilBERT), which capture semantic relationships that TF-IDF
fundamentally cannot.

\subsection{Threshold Tuning}
\label{sec:threshold}

The default classification threshold is $\tau = 0.5$.
Adjusting $\tau$ moves along the precision--recall curve without retraining:

\begin{equation}
  \hat{y} = \begin{cases}
    0 & \text{if } \hat{p}(y=1 \mid \mathbf{x}) < \tau \\
    1 & \text{otherwise}
  \end{cases}
\end{equation}

We compute the full precision--recall curve and identify:
(a) the threshold $\tau^*$ that maximises the negative-class $F_1$
    score, and
(b) the threshold where negative-class precision $\geq 0.72$
    (a business floor) at maximum recall.

This is a zero-cost post-training intervention and a practical complement
to class-imbalance handling.

\subsection{Random Forest}

A Random Forest is a bagging ensemble of $T$ decision trees,
each trained on a bootstrap sample with $m = \lfloor\sqrt{p}\rfloor$
features per split:

\begin{equation}
  \hat{P}(c \mid \mathbf{x}) = \frac{1}{T}\sum_{t=1}^{T}\hat{P}_t(c \mid \mathbf{x})
\end{equation}

Random Forest operates on dense inputs, so we use SVD-reduced features.
Hyperparameters: $T=500$, \texttt{max\_depth}=20,
\texttt{min\_samples\_split}=5, \texttt{min\_samples\_leaf}=5,
\texttt{max\_features}=`sqrt', \texttt{class\_weight}=`balanced'.

\subsection{Soft Voting Ensemble}

The Soft Voting ensemble averages class probabilities from both base models:

\begin{equation}
  \hat{P}_{\mathrm{ens}}(c \mid \mathbf{x})
  = \frac{1}{2}\Big[
    \hat{P}_{\mathrm{LR}}(c \mid \mathbf{x}) +
    \hat{P}_{\mathrm{RF}}(c \mid \mathbf{x})
  \Big]
\end{equation}

This exploits model complementarity: LR provides high negative-class recall
while RF provides high negative-class precision.

% ============================================================
\section{Experiments and Results}
\label{sec:experiments}
% ============================================================

\subsection{Evaluation Metrics}

With a 78/22 class split, accuracy is misleading.
We prioritise:

\begin{itemize}[leftmargin=2em]
  \item \textbf{Negative-class Recall}:
        $\text{Recall}_0 = \frac{TP_0}{TP_0+FN_0}$ ---
        fraction of truly negative reviews correctly identified.
  \item \textbf{Negative-class Precision}:
        $\text{Precision}_0 = \frac{TP_0}{TP_0+FP_0}$.
  \item \textbf{Negative-class F$_1$}:
        harmonic mean of precision and recall.
  \item \textbf{AUC-ROC}: threshold-independent discrimination measure.
\end{itemize}

\textbf{Business rationale}: Missing a truly negative review (false negative)
is more costly than a false alarm (false positive). A false negative means
a dissatisfied customer goes undetected; a false positive triggers a
low-cost follow-up action.

\subsection{Logistic Regression Results}

\begin{table}[H]
  \centering
  \caption{Logistic Regression results across class-imbalance strategies.
    Columns refer to the negative (minority) class.}
  \label{tab:lr}
  \setlength{\tabcolsep}{5pt}
  \begin{tabular}{lccccc}
    \toprule
    \textbf{Configuration}
      & \textbf{Accuracy}
      & \textbf{Neg. Recall}
      & \textbf{Neg. Prec.}
      & \textbf{Neg. F$_1$}
      & \textbf{AUC} \\
    \midrule
    Baseline (no balancing)          & 0.900 & 0.691 & 0.827 & 0.753 & 0.942 \\
    + Class Weights                  & 0.875 & 0.871 & 0.665 & 0.754 & 0.944 \\
    + SMOTE (SVD-reduced)            & 0.848 & 0.818 & 0.613 & 0.701 & 0.916 \\
    + Random Undersampling           & 0.870 & 0.873 & 0.648 & 0.744 & 0.941 \\
    + Class Wts.\ + ElasticNet       & 0.882 & 0.886 & 0.677 & 0.768 & 0.951 \\
    \best{+ Balanced + ElasticNet + Threshold ($\tau^*\!=\!0.37$)} & --- & \best{0.821} & \best{0.750} & \best{0.784} & \best{0.951} \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Key observations.}
The baseline achieves 90.0\% accuracy but a negative-class recall of only
0.691 --- 31\% of dissatisfied customers are missed.
Introducing class weights raises recall to 0.871 at the cost of a precision
drop from 0.827 to 0.665 (the classic precision--recall trade-off).
ElasticNet regularisation ($\alpha=0.5$, selected by 3-fold CV) raises both
recall to 0.886 and AUC to 0.951 simultaneously --- a meaningful gain
attributable to L1 sparsity removing noisy features.
\textbf{Threshold tuning at $\tau^* = 0.37$} then trades recall (0.886~$\to$~0.821)
for substantially higher precision (0.677~$\to$~0.750),
yielding the best negative-class F$_1$ of \best{0.784} among all LR variants.

\paragraph{Why SMOTE underperforms class weights.}
SMOTE operates on SVD-reduced features, which represent less than 70\%
of the original information.
The combination of information loss (SVD) and interpolation in a
compressed space degrades both precision and AUC compared to the
class-weighting approach.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.65\textwidth]{figures/fig_roc.pdf}
  \caption{ROC curves for three Logistic Regression variants. The baseline
           already achieves AUC~$\approx$~0.943; class weighting and
           ElasticNet regularisation provide incremental improvements.
           Area under each curve is reported in the legend.}
  \label{fig:roc}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_confusion.pdf}
  \caption{Confusion matrices for LR Baseline (left) and LR + Balanced +
           ElasticNet (right). Class weighting and regularisation
           substantially improve true-negative detection at the cost of
           some false positives.}
  \label{fig:confusion}
\end{figure}

\subsection{SVD Explained Variance}

Table~\ref{tab:svd} summarises the cumulative variance explained by
Truncated SVD on the TF-IDF training matrix:

\begin{table}[H]
  \centering
  \caption{Cumulative explained variance by number of SVD components.}
  \label{tab:svd}
  \begin{tabular}{cc}
    \toprule
    \textbf{Components} & \textbf{Cumulative Explained Variance} \\
    \midrule
    100   & $\approx 15\%$ \\
    300   & $30.0\%$ (measured) \\
    500   & $\approx 40\%$ \\
    1,000 & $\approx 55\%$ \\
    $>$1,000 & Required for 90\% \\
    \bottomrule
  \end{tabular}
\end{table}

This confirms that SVD is an \warn{inappropriate primary feature-extraction
strategy} for high-dimensional, sparse TF-IDF matrices.
The typical rule of thumb (top $k$ components explain $\geq 90\%$
of variance) cannot be satisfied within 1,000 components.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_svd.pdf}
  \caption{Truncated SVD variance analysis on the TF-IDF training matrix.
           Left: cumulative explained variance showing that 300 components
           explain only 30.0\% of total variance and 90\% requires more than
           1,000 components.
           Right: marginal explained variance per component with rapid
           diminishing returns after the first 50 components.}
  \label{fig:svd}
\end{figure}

\subsection{Random Forest Results}

\begin{table}[H]
  \centering
  \caption{Random Forest results. All trained on SVD-reduced features.}
  \label{tab:rf}
  \setlength{\tabcolsep}{5pt}
  \begin{tabular}{lccccc}
    \toprule
    \textbf{Configuration}
      & \textbf{Accuracy}
      & \textbf{Neg. Recall}
      & \textbf{Neg. Prec.}
      & \textbf{Neg. F$_1$}
      & \textbf{AUC} \\
    \midrule
    RF + Class Weights (SVD)    & 0.870 & 0.603 & 0.876 & 0.714 & 0.902 \\
    RF + SMOTE (SVD)            & 0.890 & 0.670 & 0.790 & 0.730 & 0.937 \\
    RF + Undersampling (SVD)    & 0.820 & 0.800 & 0.550 & 0.650 & 0.895 \\
    \bottomrule
  \end{tabular}
\end{table}

The SVD-based Random Forest achieves a notably high negative-class precision
(0.876) but a low recall (0.603).
This complementarity with the Logistic Regression motivates the ensemble.

\subsection{Ensemble Results}

\begin{table}[H]
  \centering
  \caption{All model configurations compared. \best{Green} = best value per column.}
  \label{tab:final}
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}{lccccc}
    \toprule
    \textbf{Model}
      & \textbf{Accuracy}
      & \textbf{Neg. Recall}
      & \textbf{Neg. Prec.}
      & \textbf{Neg. F$_1$}
      & \textbf{AUC} \\
    \midrule
    LR Baseline                        & 0.900          & 0.691          & 0.827          & 0.753          & 0.942 \\
    LR + Class Weights                 & 0.875          & 0.871          & 0.665          & 0.754          & 0.944 \\
    LR + SMOTE (SVD)                   & 0.848          & 0.818          & 0.613          & 0.701          & 0.916 \\
    LR + Undersampling                 & 0.870          & 0.873          & 0.648          & 0.744          & 0.941 \\
    LR + Balanced + ElasticNet         & 0.882          & 0.886          & 0.677          & 0.768          & 0.951 \\
    \best{LR + Balanced + EN + Threshold ($\tau^*\!=\!0.37$)} & ---  & 0.821          & \best{0.750}   & \best{0.784}   & 0.951 \\
    RF + Class Weights (SVD)           & 0.870          & 0.603          & \best{0.876}   & 0.714          & 0.902 \\
    RF + SMOTE (SVD)                   & 0.890          & 0.670          & 0.790          & 0.730          & 0.937 \\
    \midrule
    Soft Voting (LR + RF)              & \best{0.882}   & \best{0.844}   & 0.716          & 0.775          & \best{0.951} \\
    \bottomrule
  \end{tabular}
\end{table}

The Soft Voting ensemble achieves AUC (\best{0.951}), matching the
best single model.
It trades some precision (0.716 vs.\ 0.750 for the threshold-tuned LR)
for higher recall (0.844 vs.\ 0.821), providing a complementary operating
point useful when recall is the priority.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.92\textwidth]{figures/fig_summary.pdf}
  \caption{Comparison of key evaluation metrics across all four Logistic
           Regression model configurations (negative class). Each group of
           bars shows AUC-ROC, negative recall, negative precision, and
           negative F$_1$ for one configuration. Threshold tuning (\textit{+Threshold})
           improves the precision--recall balance without retraining.}
  \label{fig:summary}
\end{figure}

\subsection{Threshold Tuning Analysis}

Figure~\ref{fig:pr} shows the full operating range for the best LR model.
The optimal threshold $\tau^* = 0.37$ maximises negative-class F$_1$,
improving precision from 0.677 (default $\tau=0.5$) to \textbf{0.750}
while reducing recall from 0.886 to 0.821.
The net result is a higher F$_1$ of \textbf{0.784} vs.\ 0.768 at default threshold.
A lower threshold $\tau < 0.37$ pushes recall above 0.88 at the expense
of precision dropping below 0.67, suitable for aggressive complaint detection.

\textbf{Practical implication}: These two operating points correspond to
two different business modes ---
aggressive complaint detection (lower $\tau$, higher recall) vs.\
precision-focused flagging (higher $\tau$, fewer false alarms).
The deployment team can select the appropriate mode without model retraining.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_pr.pdf}
  \caption{Left: Precision--Recall curve for the negative (minority) class
           using the best LR model. The red dot marks the threshold
           $\tau^*$ that maximises negative-class F$_1$. The dashed green
           line shows a precision floor of 0.72.
           Right: F$_1$ score vs.\ classification threshold $\tau$, showing
           the optimal operating point.}
  \label{fig:pr}
\end{figure}

\subsection{Feature Importance}

Table~\ref{tab:features} lists the features with the largest absolute
coefficients from the best LR model.

\begin{table}[H]
  \centering
  \caption{Top sentiment-predictive TF-IDF features from Logistic Regression.}
  \label{tab:features}
  \setlength{\tabcolsep}{6pt}
  \begin{tabular}{lc|lc}
    \toprule
    \multicolumn{2}{c|}{\textbf{Positive Features}} &
    \multicolumn{2}{c}{\textbf{Negative Features}} \\
    \textbf{Feature} & \textbf{Coef.} & \textbf{Feature} & \textbf{Coef.} \\
    \midrule
    four star         & $+11.2$ & three star        & $-13.1$ \\
    perfect           & $+11.0$ & worst             & $-12.4$ \\
    won't disappoint  & $+10.7$ & won't buy         & $-10.5$ \\
    highly recommend  & $+10.7$ & disappoint        & $-10.4$ \\
    delicious         & $+10.6$ & two star          & $-10.4$ \\
    great             & $+9.7$  & unfortunate       & $-9.4$ \\
    excellent         & $+9.4$  & terrible          & $-8.2$ \\
    love              & $+9.3$  & mediocre          & $-7.2$ \\
    best              & $+9.2$  & disgusting        & $-6.8$ \\
    \bottomrule
  \end{tabular}
\end{table}

Two notable patterns emerge.
First, star-count bigrams (\textit{four star}, \textit{three star})
dominate both polarities, indicating that reviewers often verbalise their
numeric rating.
Second, negation-phrase bigrams (\textit{won't disappoint},
\textit{won't buy}) appear in the top coefficients of both classes ---
confirming that retaining negation words during preprocessing
is essential for capturing these high-value features.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_features.pdf}
  \caption{Top 15 TF-IDF bi-gram features by absolute Logistic Regression
           coefficient. Positive-sentiment features (left, green) and
           negative-sentiment features (right, red). Note the dominance
           of star-count bigrams and negation phrases.}
  \label{fig:features}
\end{figure}

% ============================================================
\section{Discussion}
\label{sec:discussion}
% ============================================================

\subsection{Stop-Word Handling is a Critical Design Decision}

Our empirical results confirm the theoretical argument: removing negation
words inverts the polarity of common sentiment phrases.
The presence of bigrams such as \textit{won't buy} (coefficient $-10.5$)
and \textit{won't disappoint} (coefficient $+10.7$) in the top feature
importance list validates the importance of retaining these words.
Future work should explore more sophisticated negation handling,
such as tagging the $k$ words following a negation token with a negation
marker \citep{pang2002thumbs}.

\subsection{SVD is Not a Universal Improvement}

The prevailing intuition that ``lower dimensionality = better generalisation''
does not hold for NLP TF-IDF features.
The high sparsity and large vocabulary mean that each SVD component captures
only a tiny fraction of the total variance.
Compressing to 1,000 components loses approximately 45\% of the information
before any model is trained.
This explains why our SMOTE-based models (which require SVD for dense input)
generally underperform class-weighting-based models that operate on the
full sparse TF-IDF matrix.

\subsection{Threshold Tuning vs.\ Class Imbalance Handling}

Threshold tuning and class imbalance handling address the same underlying
problem (biased predictions toward the majority class) through different
mechanisms.
Class imbalance handling modifies the training distribution or loss function;
threshold tuning adjusts the decision rule post-training.
They are complementary and both should be applied in practice.

\subsection{Limitations}

\begin{itemize}[leftmargin=2em]
  \item \textbf{No word embeddings}: TF-IDF treats vocabulary as a flat
        set of unordered tokens. Contextual embeddings (BERT, RoBERTa)
        would capture semantic similarity and long-range negation context
        that TF-IDF fundamentally cannot.
  \item \textbf{Neutral class excluded}: Reviews with score~3 are discarded.
        A production system should handle neutral sentiment.
  \item \textbf{Temporal drift}: The 13-year span introduces concept drift;
        vocabulary and sentiment expression evolve over time.
  \item \textbf{Domain specificity}: Models may not generalise across
        product categories without retraining.
\end{itemize}

% ============================================================
\section{Conclusion}
\label{sec:conclusion}
% ============================================================

We presented a principled NLP pipeline for binary sentiment classification
of Amazon Fine Food Reviews, with an emphasis on correcting common
methodological mistakes and providing interpretable, reproducible results.

Our main findings are:

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Keep negation stop words.}
        Standard stop-word removal degrades sentiment classification
        by discarding semantically critical negation words.
        Always use a custom stop-word list that excludes negation tokens.

  \item \textbf{Build a baseline before applying complexity.}
        The baseline LR on raw TF-IDF already achieves AUC 0.942.
        Every subsequent step's contribution can then be measured precisely.

  \item \textbf{SVD is inappropriate as primary dimensionality reduction
        for NLP.}
        300 components explain only $\approx$30\% of variance;
        $>$1,000 components are needed for 90\%. Use SVD only when
        dense input is required by a downstream algorithm (e.g., SMOTE).

  \item \textbf{Class weighting dominates other imbalance strategies.}
        It is computationally free, operates on the full sparse matrix,
        and consistently achieves the best recall--precision balance.

  \item \textbf{Threshold tuning is a zero-cost deployment strategy.}
        Adjusting $\tau$ after training allows flexible precision--recall
        trade-offs without model retraining.

  \item \textbf{Best single model: LR + ElasticNet + Threshold ($\tau^*=0.37$)}
        achieves NegF$_1$=0.784, NegPrec=0.750, NegRec=0.821, AUC=0.951.
        Soft Voting matches the AUC (0.951) with higher recall (0.844)
        at the cost of lower precision (0.716), useful for different deployment modes.

  \item \textbf{Future recommendation}: Fine-tune DistilBERT or RoBERTa
        on this dataset to capture contextual negation and domain vocabulary,
        expected to achieve AUC $>0.97$.
\end{enumerate}

% ============================================================
\newpage
\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Baccianella et al.(2010)]{baccianella2010sentiwordnet}
Baccianella, S., Esuli, A., and Sebastiani, F. (2010).
\newblock SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis
  and opinion mining.
\newblock In \textit{Proceedings of LREC}, volume~10, pages 2200--2204.

\bibitem[Chawla et al.(2002)]{chawla2002smote}
Chawla, N.~V., Bowyer, K.~W., Hall, L.~O., and Kegelmeyer, W.~P. (2002).
\newblock SMOTE: Synthetic minority over-sampling technique.
\newblock \textit{Journal of Artificial Intelligence Research}, 16:321--357.

\bibitem[Devlin et al.(2019)]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).
\newblock BERT: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \textit{Proceedings of NAACL-HLT}, pages 4171--4186.

\bibitem[Hutto and Gilbert(2014)]{hutto2014vader}
Hutto, C. and Gilbert, E. (2014).
\newblock VADER: A parsimonious rule-based model for sentiment analysis of
  social media text.
\newblock In \textit{Proceedings of ICWSM}.

\bibitem[Maas et al.(2011)]{maas2011learning}
Maas, A.~L., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
  (2011).
\newblock Learning word vectors for sentiment analysis.
\newblock In \textit{Proceedings of ACL-HLT}, pages 142--150.

\bibitem[McAuley and Leskovec(2013)]{mcauley2013hidden}
McAuley, J.~J. and Leskovec, J. (2013).
\newblock Hidden factors and hidden topics: Understanding rating dimensions
  with review text.
\newblock In \textit{Proceedings of RecSys}, pages 165--172.

\bibitem[Pang and Lee(2008)]{pang2008opinion}
Pang, B. and Lee, L. (2008).
\newblock Opinion mining and sentiment analysis.
\newblock \textit{Foundations and Trends in Information Retrieval},
  2(1--2):1--135.

\bibitem[Pang et al.(2002)]{pang2002thumbs}
Pang, B., Lee, L., and Vaithyanathan, S. (2002).
\newblock Thumbs up? Sentiment classification using machine learning
  techniques.
\newblock In \textit{Proceedings of EMNLP}, pages 79--86.

\end{thebibliography}

\end{document}
