\documentclass[11pt,a4paper]{article}

% ============================================================
%  Packages
% ============================================================
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{natbib}
\usepackage{microtype}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{abstract}

% ============================================================
%  Hyperref setup
% ============================================================
\hypersetup{
  colorlinks=true,
  linkcolor=blue!70!black,
  citecolor=green!50!black,
  urlcolor=blue!70!black
}

% ============================================================
%  Header / Footer
% ============================================================
\pagestyle{fancy}
\fancyhf{}
\rhead{\small Amazon Fine Food Review — Sentiment Analysis}
\lhead{\small NLP Project Report}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ============================================================
%  Section title formatting
% ============================================================
\titleformat{\section}{\large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection.}{0.5em}{}
\titleformat{\subsubsection}{\normalsize\itshape}{\thesubsubsection.}{0.5em}{}

% ============================================================
%  Custom commands
% ============================================================
\newcommand{\F}[1]{\textit{F$_1$-score}}
\newcommand{\model}[1]{\textsc{#1}}
\definecolor{bestcolor}{RGB}{0,100,0}

% ============================================================
%  Document
% ============================================================
\begin{document}

% ---- Title block ----
\begin{titlepage}
  \centering
  \vspace*{2cm}

  {\LARGE\bfseries Sentiment Analysis of Amazon Fine Food Reviews:\\
  A Comparative Study of Logistic Regression, Random Forest,\\
  and Ensemble Methods under Class Imbalance\par}

  \vspace{1.5cm}

  {\large Ricky\par}

  \vspace{0.5cm}

  {\normalsize Data Science Program\par}

  \vspace{0.5cm}

  {\normalsize \today\par}

  \vspace{2cm}

  \begin{abstract}
    \noindent
    This paper presents a complete machine learning pipeline for sentiment
    classification of Amazon Fine Food Reviews — a large-scale corpus of
    568,454 user-generated reviews spanning food products sold on Amazon.
    Given only the review text, our system predicts whether the expressed
    opinion is \textit{positive} or \textit{negative}.
    The main technical challenges addressed are (1) high-dimensional sparse
    text representations, (2) significant class imbalance (78\% positive vs.\
    22\% negative), and (3) the business requirement of maximising recall
    for the minority negative class.
    We explore three vectorisation schemes (Bag-of-Words, Bi-gram BOW,
    TF-IDF) combined with Truncated Singular Value Decomposition (SVD) for
    dimensionality reduction, three imbalance-handling strategies (class
    weighting, SMOTE, and random undersampling), and two base classifiers
    (Logistic Regression with ElasticNet regularisation and Random Forest).
    Our best single model — a balanced, regularised Logistic Regression —
    achieves an AUC of 0.945.
    A Soft Voting ensemble that combines the complementary strengths of both
    base classifiers achieves the highest overall AUC of \textbf{0.951} with
    a negative-class recall of 0.844 and precision of 0.716, striking a
    favourable balance between sensitivity and specificity for practical
    deployment.
  \end{abstract}

  \vspace{1cm}

  {\small\textbf{Keywords:} sentiment analysis, natural language processing,
    TF-IDF, logistic regression, random forest, ensemble learning,
    class imbalance, SMOTE, Amazon reviews}

\end{titlepage}

% ---- Table of Contents ----
\tableofcontents
\newpage

% ============================================================
\section{Introduction}
% ============================================================

Online product reviews have become a primary information source for both
consumers and businesses. The sheer volume of user-generated text — hundreds
of millions of reviews on platforms such as Amazon — makes manual inspection
infeasible, motivating the development of automated sentiment analysis systems.

\textit{Sentiment analysis} (also called opinion mining) aims to identify and
extract subjective information from text, categorising it into polarities such
as positive or negative \citep{pang2008opinion}.  Applications span a broad
spectrum: brand monitoring, product quality control, recommender systems,
vendor selection, and customer-experience personalisation.

This work focuses on the \textbf{Amazon Fine Food Reviews} dataset, a
well-known benchmark comprising 568,454 reviews from 1999 to 2012.
Each review is rated on a 1–5 star scale.  We simplify the task to binary
classification: scores above 3 are \textit{positive}; scores below 3 are
\textit{negative}; scores equal to 3 are excluded to avoid label ambiguity.

The central contributions of this paper are:
\begin{itemize}[leftmargin=2em]
  \item A systematic comparison of text vectorisation strategies (BOW,
        bi-gram BOW, TF-IDF) and dimensionality reduction via SVD.
  \item A comprehensive evaluation of three class-imbalance mitigation
        techniques under a recall-priority business objective.
  \item An ensemble strategy (Soft Voting) that reconciles the high-precision
        Random Forest with the high-recall Logistic Regression, achieving
        state-of-the-art AUC on the dataset.
  \item Feature-importance analysis that provides interpretable linguistic
        insight into the drivers of positive and negative sentiment.
\end{itemize}

The remainder of the paper is organised as follows.
Section~\ref{sec:related} reviews related work.
Section~\ref{sec:data} describes the dataset and exploratory analysis.
Section~\ref{sec:method} details the full modelling pipeline.
Section~\ref{sec:experiments} presents experimental results.
Section~\ref{sec:discussion} discusses key findings and limitations.
Section~\ref{sec:conclusion} concludes.

% ============================================================
\section{Related Work}
\label{sec:related}
% ============================================================

Early sentiment analysis research relied on lexicon-based methods,
using manually compiled sentiment dictionaries such as SentiWordNet
\citep{baccianella2010sentiwordnet} and VADER \citep{hutto2014vader}.
These methods are interpretable but brittle in domain-specific contexts.

\citet{pang2002thumbs} pioneered machine learning approaches,
demonstrating that Naive Bayes, SVM, and maximum entropy classifiers
outperform human-crafted rules on movie reviews.
\citet{maas2011learning} introduced the IMDb dataset and showed that
distributional word vectors capture semantic sentiment structure.

For Amazon product reviews specifically, \citet{mcauley2013hidden}
proposed latent factor models that jointly model user preferences and
sentiment. More recent work applies Transformer-based models such as
BERT \citep{devlin2019bert} and RoBERTa, achieving near-human performance
on fine-grained sentiment tasks.

This project deliberately focuses on \textit{classical} machine learning
methods — Logistic Regression and Random Forest — because (1) they are
computationally tractable on commodity hardware with datasets of this size,
(2) they provide strong baselines, and (3) their feature importance scores
are directly interpretable, which is valuable in business settings.

% ============================================================
\section{Dataset and Exploratory Analysis}
\label{sec:data}
% ============================================================

\subsection{Dataset Description}

The Amazon Fine Food Reviews dataset\footnote{%
  \url{https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews}}
contains 568,454 reviews of food products sold on Amazon, spanning October
1999 to October 2012. Each record includes the fields shown in
Table~\ref{tab:features}.

\begin{table}[H]
  \centering
  \caption{Dataset features and descriptions.}
  \label{tab:features}
  \begin{tabular}{llp{7cm}}
    \toprule
    \textbf{Field} & \textbf{Type} & \textbf{Description} \\
    \midrule
    Id              & Integer   & Unique review identifier \\
    ProductId       & String    & Amazon ASIN of the product \\
    UserId          & String    & Amazon user identifier \\
    ProfileName     & String    & User display name \\
    VotesHelpful    & Integer   & Number of helpful votes \\
    VotesTotal      & Integer   & Total votes \\
    Score           & Integer   & Star rating (1–5) \\
    Time            & Unix time & Review submission timestamp \\
    Summary         & String    & Short review title \\
    Text            & String    & Full review body \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Label Construction}

We construct binary labels as follows:
\begin{equation}
  y_i =
  \begin{cases}
    1 \;(\text{positive}) & \text{if } \text{Score}_i > 3 \\
    0 \;(\text{negative}) & \text{if } \text{Score}_i < 3 \\
    \text{excluded}       & \text{if } \text{Score}_i = 3
  \end{cases}
\end{equation}
After exclusion, 443,777 reviews are positive and 124,677 are negative,
yielding a \textbf{78.1\% / 21.9\%} class split — a moderate but
non-trivial imbalance.

\subsection{Exploratory Analysis}

Figure~\ref{fig:wordcloud} (not reproduced here for brevity) shows word
clouds of cleaned review summaries stratified by label.
High-scoring reviews are dominated by terms such as \textit{great},
\textit{love}, \textit{best}, and \textit{delicious}, while low-scoring
reviews feature \textit{terrible}, \textit{worst}, \textit{disappoint},
and \textit{return}.
This lexical separation confirms that bag-of-words features carry
sufficient discriminative signal for classification.

% ============================================================
\section{Methodology}
\label{sec:method}
% ============================================================

\subsection{Text Preprocessing}

Raw review text undergoes the following cleaning steps:

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Lowercasing}: All characters converted to lower case.
  \item \textbf{Punctuation removal}: Most punctuation replaced with
        whitespace; apostrophes and quotation marks deleted.
  \item \textbf{Tokenisation}: Simple whitespace splitting.
  \item \textbf{Stemming}: Porter Stemmer (solution notebook) /
        Snowball Stemmer (course template) applied to reduce inflected
        forms to their root.
  \item \textbf{Stop-word handling}: In the course template, standard
        NLTK stop words are \textit{retained} because negation words such
        as \textit{not}, \textit{won't}, and \textit{couldn't} carry strong
        sentiment polarity that would be lost by removal.
        The solution notebook applies stop-word removal as an ablation.
\end{enumerate}

\subsection{Feature Engineering}
\label{sec:features}

\subsubsection{Bag-of-Words (BOW)}
A document-term matrix is constructed using binary term presence
(\texttt{CountVectorizer}, \texttt{min\_df=5}) for both unigrams and bigrams.
The bi-gram vocabulary reaches 46,995 features.

\subsubsection{TF-IDF}
Term Frequency–Inverse Document Frequency weighting downweights
ubiquitous terms and upweights discriminative ones:
\begin{equation}
  \text{tfidf}(t, d, D) = \text{tf}(t,d) \cdot \log\!\left(\frac{|D|}{|\{d'\in D : t\in d'\}|}\right)
\end{equation}
We use unigram + bigram TF-IDF with \texttt{max\_features=10{,}000},
yielding a sparse matrix of shape $(568{,}454,\,10{,}000)$.

\subsubsection{Truncated SVD (LSA)}
Because tree-based models such as Random Forest cannot operate efficiently
on sparse high-dimensional matrices, and to enable SMOTE (which requires
dense input), we apply Truncated SVD — a form of Latent Semantic Analysis —
to reduce the TF-IDF matrix from 10,000 to \textbf{1,000 dimensions}:
\begin{equation}
  \mathbf{X}_{\text{reduced}} = \mathbf{U}_{:,1:k}\,\boldsymbol{\Sigma}_{1:k,1:k}
\end{equation}
where $k=1000$.  The SVD is fitted on the training split only and
applied to the test split to prevent data leakage.

\subsection{Train/Test Split}

The dataset is split \textbf{80\% train / 20\% test} using stratified
sampling (\texttt{random\_state=42}) to preserve the label ratio.
All preprocessing (SVD, SMOTE, undersampling) is fitted exclusively on
the training partition.

\subsection{Handling Class Imbalance}
\label{sec:imbalance}

Three strategies are evaluated against the baseline (no balancing):

\paragraph{Strategy 1 — Class Weights.}
The loss function is re-weighted inversely proportional to class frequency,
effectively penalising misclassification of the minority class more heavily:
\begin{equation}
  w_c = \frac{N}{K \cdot N_c}
\end{equation}
where $N$ is total samples, $K$ the number of classes, and $N_c$ samples
in class $c$.  This incurs no additional computation and preserves the
original data distribution.

\paragraph{Strategy 2 — SMOTE.}
The Synthetic Minority Over-sampling Technique \citep{chawla2002smote}
generates synthetic minority-class samples by interpolating between
existing minority instances in feature space.
Applied after SVD reduction, it equalises class counts in the training set.

\paragraph{Strategy 3 — Random Undersampling.}
Majority-class samples are randomly discarded until class parity is reached.
Simple and fast, but risks discarding informative majority-class samples.

\subsection{Classifiers}

\subsubsection{Logistic Regression}
We train an $\ell_1$/$\ell_2$ ElasticNet regularised logistic regression:
\begin{equation}
  \min_{\boldsymbol\theta} \sum_{i=1}^{N} \mathcal{L}(y_i, \sigma(\boldsymbol\theta^\top\mathbf{x}_i))
  + \lambda\!\left[\alpha\|\boldsymbol\theta\|_1 + (1-\alpha)\|\boldsymbol\theta\|_2^2\right]
\end{equation}
where $\alpha$ (the \texttt{l1\_ratio}) is tuned via 3-fold cross-validation
over $\{0.1, 0.3, 0.5, 0.7, 0.9\}$.
The scoring criterion is \textit{negative-class recall}.
The optimal $\alpha = 0.5$ (equal $\ell_1$/$\ell_2$ mixture).

\subsubsection{Random Forest}
A Random Forest of $T$ decision trees, each trained on a bootstrap sample
with $\sqrt{p}$ randomly selected features per split:
\begin{equation}
  \hat{y} = \operatorname*{argmax}_c \frac{1}{T}\sum_{t=1}^{T} \hat{p}_t(c\mid\mathbf{x})
\end{equation}
Hyperparameters are tuned via 5-fold GridSearchCV:
\texttt{n\_estimators} $\in \{100, 300, 500\}$,
\texttt{max\_depth} $\in \{10, 20\}$,
\texttt{min\_samples\_split} $\in \{5, 10, 20\}$,
\texttt{min\_samples\_leaf} $\in \{5, 10, 20\}$.
Best configuration: 500 trees, depth 20, \texttt{min\_samples\_split=5},
\texttt{min\_samples\_leaf=5}.

\subsubsection{Soft Voting Ensemble}
The ensemble averages the class probability outputs of both base models:
\begin{equation}
  P_{\text{ens}}(c\mid\mathbf{x}) = \frac{1}{2}\!\left[P_{\text{LR}}(c\mid\mathbf{x}) + P_{\text{RF}}(c\mid\mathbf{x})\right]
\end{equation}
This exploits complementarity: Logistic Regression provides higher
negative recall while Random Forest provides higher negative precision.

\subsection{Evaluation Metrics}

Given the class imbalance and recall-priority objective, we report:

\begin{itemize}[leftmargin=2em]
  \item \textbf{Precision} and \textbf{Recall} for the negative (minority) class.
  \item \textbf{F$_1$-score}: harmonic mean of precision and recall.
  \item \textbf{AUC-ROC}: area under the receiver operating characteristic curve —
        threshold-independent measure of discrimination.
  \item \textbf{Overall Accuracy}: reported for completeness but not the primary criterion.
\end{itemize}

% ============================================================
\section{Experiments and Results}
\label{sec:experiments}
% ============================================================

\subsection{Logistic Regression Results}

Table~\ref{tab:lr_results} summarises the Logistic Regression experiments.

\begin{table}[H]
  \centering
  \caption{Logistic Regression results on the test set.
           Neg.\ = negative class.}
  \label{tab:lr_results}
  \setlength{\tabcolsep}{6pt}
  \begin{tabular}{lccccc}
    \toprule
    \textbf{Configuration} & \textbf{Accuracy} & \textbf{Neg.\ Recall} & \textbf{Neg.\ Prec.} & \textbf{Neg.\ F$_1$} & \textbf{AUC} \\
    \midrule
    Baseline (no balancing)              & 0.902 & 0.691 & 0.828 & 0.753 & 0.943 \\
    + Class weight (balanced)            & 0.875 & 0.872 & 0.661 & 0.752 & 0.944 \\
    + SMOTE (SVD-reduced)                & 0.848 & 0.818 & 0.613 & 0.701 & 0.916 \\
    + Random undersampling               & 0.870 & 0.873 & 0.648 & 0.744 & 0.941 \\
    \textbf{+ Balanced + ElasticNet}     & \textbf{0.877} & \textbf{0.875} & \textbf{0.665} & \textbf{0.755} & \textbf{0.945} \\
    \bottomrule
  \end{tabular}
\end{table}

The baseline achieves a high overall accuracy of 90.2\%, but the negative-class
recall of 0.691 is unacceptably low for the target business use case.
Introducing class weighting raises recall to 0.872 at the cost of a precision
drop from 0.828 to 0.661 — an expected precision–recall trade-off.
ElasticNet regularisation with $\alpha=0.5$ marginally improves both recall and
AUC, yielding our best single LR model.

\subsection{Random Forest Results}

\begin{table}[H]
  \centering
  \caption{Random Forest results on the test set.}
  \label{tab:rf_results}
  \setlength{\tabcolsep}{6pt}
  \begin{tabular}{lccccc}
    \toprule
    \textbf{Configuration} & \textbf{Accuracy} & \textbf{Neg.\ Recall} & \textbf{Neg.\ Prec.} & \textbf{Neg.\ F$_1$} & \textbf{AUC} \\
    \midrule
    RF + class weight (full TF-IDF)      & 0.832 & 0.785 & 0.584 & 0.670 & 0.902 \\
    RF + class weight + SVD              & 0.870 & 0.603 & 0.876 & 0.714 & 0.902 \\
    RF + SMOTE (SVD-reduced)             & 0.890 & 0.670 & 0.790 & 0.730 & 0.937 \\
    RF + random undersampling            & 0.820 & 0.800 & 0.550 & 0.650 & 0.895 \\
    \bottomrule
  \end{tabular}
\end{table}

After SVD reduction and class weighting, the Random Forest achieves a notably
high negative-class precision of 0.876 — the highest among all single models —
but its recall drops to 0.603.
This precision–recall complementarity with the Logistic Regression motivates
the ensemble approach.

\subsection{Soft Voting Ensemble Results}

\begin{table}[H]
  \centering
  \caption{Comparison of all model configurations. Best values in \textcolor{bestcolor}{\textbf{green}}.}
  \label{tab:final_comparison}
  \setlength{\tabcolsep}{5pt}
  \begin{tabular}{lccccc}
    \toprule
    \textbf{Model} & \textbf{Accuracy} & \textbf{Neg.\ Recall} & \textbf{Neg.\ Prec.} & \textbf{Neg.\ F$_1$} & \textbf{AUC} \\
    \midrule
    LR Baseline                          & 0.902 & 0.691 & 0.828 & 0.753 & 0.943 \\
    LR + Balanced + ElasticNet           & 0.877 & 0.875 & 0.665 & 0.755 & 0.945 \\
    RF + Class Weight + SVD              & 0.870 & 0.603 & 0.876 & 0.714 & 0.902 \\
    RF + SMOTE                           & 0.890 & 0.670 & 0.790 & 0.730 & 0.937 \\
    \midrule
    \textbf{Soft Voting (LR + RF)}       &
      \textcolor{bestcolor}{\textbf{0.882}} &
      \textcolor{bestcolor}{\textbf{0.844}} &
      \textcolor{bestcolor}{\textbf{0.716}} &
      \textcolor{bestcolor}{\textbf{0.775}} &
      \textcolor{bestcolor}{\textbf{0.951}} \\
    \bottomrule
  \end{tabular}
\end{table}

The Soft Voting ensemble achieves the highest AUC of \textbf{0.951}, outperforming
both base models.  Negative-class recall (0.844) is substantially higher than
the SVD Random Forest (0.603) while negative-class precision (0.716) improves
over the balanced Logistic Regression (0.665).
The ensemble thus successfully exploits model complementarity.

\subsection{Feature Importance Analysis}

\subsubsection{Logistic Regression Coefficients}

Table~\ref{tab:lr_features} lists the features with the largest positive and
negative coefficients from the best Logistic Regression model.

\begin{table}[H]
  \centering
  \caption{Top sentiment-predictive features from Logistic Regression.}
  \label{tab:lr_features}
  \begin{tabular}{lc|lc}
    \toprule
    \multicolumn{2}{c|}{\textbf{Positive Sentiment}} &
    \multicolumn{2}{c}{\textbf{Negative Sentiment}} \\
    \textbf{Feature} & \textbf{Coef.} & \textbf{Feature} & \textbf{Coef.} \\
    \midrule
    four star        & +11.21 & three star       & $-$13.06 \\
    perfect          & +10.97 & worst            & $-$12.37 \\
    won't disappoint & +10.67 & won't buy        & $-$10.47 \\
    highly recommend & +10.65 & disappoint       & $-$10.43 \\
    delicious        & +10.55 & two star         & $-$10.36 \\
    great            & $+$9.73 & unfortunate      & $-$9.40 \\
    excellent        & $+$9.35 & terrible         & $-$8.16 \\
    love             & $+$9.29 & mediocre         & $-$7.24 \\
    best             & $+$9.16 & disgusting       & $-$6.82 \\
    \bottomrule
  \end{tabular}
\end{table}

Several noteworthy patterns emerge.
First, star-count bigrams (\textit{four star}, \textit{three star},
\textit{two star}) dominate both polarities, suggesting that reviewers often
explicitly verbalise their numeric rating.
Second, expectation-violation phrases (\textit{won't disappoint},
\textit{highly recommend} vs.\ \textit{won't buy}, \textit{disappoint})
capture user intent beyond simple adjective polarity.

\subsubsection{Random Forest Feature Importances}

The Random Forest assigns the highest importance scores to: \textit{not},
\textit{great}, \textit{best}, \textit{delicious}, \textit{love},
\textit{disappoint}, and \textit{but}.
The prominence of \textit{not} confirms that retaining negation tokens
during preprocessing is critical for model performance.

% ============================================================
\section{Discussion}
\label{sec:discussion}
% ============================================================

\subsection{Choice of Evaluation Metric}

In this application, missing a genuinely negative review (false negative)
is more costly than generating a false alarm (false positive):
a false negative means the business fails to detect a dissatisfied customer,
while a false positive merely triggers an unnecessary follow-up action.
This asymmetric cost structure justifies prioritising \textbf{negative-class
recall} over overall accuracy, a decision reflected throughout our model
selection process.

\subsection{Stop-Word Removal}

Our experiments confirm the folklore that stop-word removal can be
\textit{harmful} for sentiment classification: negation words such as
\textit{not}, \textit{won't}, and \textit{can't} are standard stop words
but convey critical polarity-flipping information.
This is consistent with findings in \citet{pang2002thumbs}.

\subsection{SVD and Information Loss}

Compressing the 10,000-dimensional TF-IDF space to 1,000 dimensions via
SVD enables SMOTE and improves Random Forest efficiency.
However, it slightly reduces discriminative power for Logistic Regression:
the full-TF-IDF LR (AUC 0.945) outperforms the SVD-LR (AUC 0.916).
The optimal strategy is therefore model-dependent.

\subsection{Limitations}

\begin{itemize}[leftmargin=2em]
  \item \textbf{Neutral class exclusion}: Reviews with Score = 3 are discarded.
        A production system should handle neutral sentiment.
  \item \textbf{Temporal drift}: The dataset spans 13 years; vocabulary and
        sentiment expression may shift over time (concept drift).
  \item \textbf{Domain specificity}: Models trained on food reviews may
        not generalise to other product categories without retraining.
  \item \textbf{No deep learning baseline}: Transformer models such as BERT
        would likely yield higher performance but require substantially greater
        compute resources.
\end{itemize}

% ============================================================
\section{Conclusion}
\label{sec:conclusion}
% ============================================================

We presented a comprehensive NLP pipeline for binary sentiment classification
of the Amazon Fine Food Reviews corpus, addressing key practical challenges
including high-dimensional text representation, class imbalance, and the
precision–recall trade-off.

Our main findings are:
\begin{enumerate}[leftmargin=2em]
  \item \textbf{TF-IDF with bi-grams} is the most informative vectorisation
        strategy for this task, capturing phrasal sentiment signals that
        unigrams miss.
  \item \textbf{Class weighting} is the most effective and computationally
        efficient imbalance-handling strategy, outperforming SMOTE and
        undersampling in most settings.
  \item \textbf{ElasticNet regularisation} provides a marginal but consistent
        improvement over $\ell_2$-only regularisation for Logistic Regression.
  \item The \textbf{Soft Voting ensemble} (Logistic Regression + Random Forest)
        achieves the best overall AUC (0.951) by combining high-recall and
        high-precision base models — an effective strategy when the two
        classifiers are complementary.
  \item \textbf{Retaining negation stop words} is important for sentiment
        classification; standard stop-word lists should be customised for
        NLP applications.
\end{enumerate}

Future work should explore fine-tuned Transformer models (BERT, RoBERTa)
as a stronger baseline, and investigate threshold calibration for deployment
under varying precision–recall requirements.

% ============================================================
%  References
% ============================================================
\newpage
\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Baccianella et al.(2010)]{baccianella2010sentiwordnet}
Baccianella, S., Esuli, A., and Sebastiani, F. (2010).
\newblock SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis
  and opinion mining.
\newblock In \textit{Proceedings of LREC}, volume~10, pages 2200--2204.

\bibitem[Chawla et al.(2002)]{chawla2002smote}
Chawla, N.~V., Bowyer, K.~W., Hall, L.~O., and Kegelmeyer, W.~P. (2002).
\newblock SMOTE: Synthetic minority over-sampling technique.
\newblock \textit{Journal of Artificial Intelligence Research}, 16:321--357.

\bibitem[Devlin et al.(2019)]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).
\newblock BERT: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \textit{Proceedings of NAACL-HLT}, pages 4171--4186.

\bibitem[Hutto and Gilbert(2014)]{hutto2014vader}
Hutto, C. and Gilbert, E. (2014).
\newblock VADER: A parsimonious rule-based model for sentiment analysis of social
  media text.
\newblock In \textit{Proceedings of ICWSM}.

\bibitem[Maas et al.(2011)]{maas2011learning}
Maas, A.~L., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
  (2011).
\newblock Learning word vectors for sentiment analysis.
\newblock In \textit{Proceedings of ACL-HLT}, pages 142--150.

\bibitem[McAuley and Leskovec(2013)]{mcauley2013hidden}
McAuley, J.~J. and Leskovec, J. (2013).
\newblock Hidden factors and hidden topics: Understanding rating dimensions with
  review text.
\newblock In \textit{Proceedings of RecSys}, pages 165--172.

\bibitem[Pang and Lee(2008)]{pang2008opinion}
Pang, B. and Lee, L. (2008).
\newblock Opinion mining and sentiment analysis.
\newblock \textit{Foundations and Trends in Information Retrieval},
  2(1--2):1--135.

\bibitem[Pang et al.(2002)]{pang2002thumbs}
Pang, B., Lee, L., and Vaithyanathan, S. (2002).
\newblock Thumbs up? Sentiment classification using machine learning techniques.
\newblock In \textit{Proceedings of EMNLP}, pages 79--86.

\end{thebibliography}

\end{document}
