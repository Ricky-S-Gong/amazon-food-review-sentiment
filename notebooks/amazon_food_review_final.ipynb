{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": "# Amazon Fine Food Reviews \u2014 Sentiment Analysis\n\n**Author:** Ricky Gong, University of Pennsylvania (`gong8@seas.upenn.edu`)  \n**Dataset:** Amazon Fine Food Reviews (568,454 reviews)  \n**Task:** Binary sentiment classification \u2014 Positive vs. Negative\n\n---\n\n## Project Overview\n\nThis notebook builds a complete NLP pipeline to predict the **sentiment** (positive / negative) of Amazon food product reviews. Starting from raw text, we apply text preprocessing, feature engineering, and progressively more sophisticated classification models, addressing the key challenges of **class imbalance**, the **precision\u2013recall trade-off**, and the **expressiveness of text representations**.\n\n### Key Design Principles\n\n1. **Baseline-first**: Always train a baseline model before adding any complexity.\n2. **Stop-word caution**: In sentiment analysis, negation words (\"not\", \"won't\", \"can't\") must **not** be removed.\n3. **Dimensionality reduction is not free**: SVD is appropriate only when dense input is required (e.g., for SMOTE), never as the first step before evaluation.\n4. **Evaluation beyond Accuracy**: With a 78/22 class split, we prioritise **Recall** for the negative (minority) class.\n5. **Representation matters**: We compare sparse TF-IDF, static dense Word2Vec, and contextual transformer (DistilBERT) representations.\n\n> \u26a1 **GPU Note**: Sections 1\u201310 and 11.1\u201311.2 run on CPU. **Section 11.3 (DistilBERT) requires a GPU.** Run on Google Colab (Runtime \u2192 Change Runtime Type \u2192 GPU).\n\n---\n\n## Table of Contents\n\n1. [Setup & Imports](#1)\n2. [Data Loading & Exploratory Analysis](#2)\n3. [Text Preprocessing](#3)\n4. [Feature Engineering \u2014 TF-IDF](#4)\n5. [Train / Test Split](#5)\n6. [Baseline Model \u2014 Logistic Regression](#6)\n7. [Class Imbalance: Problem & Solutions](#7)\n   - 7.1 Class Weights\n   - 7.2 SMOTE (with SVD critique)\n   - 7.3 Random Undersampling\n8. [Regularisation & Hyperparameter Tuning](#8)\n9. [Threshold Tuning \u2014 Balancing Precision & Recall](#9)\n10. [Random Forest Classifier](#10)\n11. [Beyond Bag-of-Words: Word Embeddings](#11)\n    - 11.1 Theory and Motivation\n    - 11.2 Word2Vec \u2014 Static Dense Embeddings\n    - 11.3 DistilBERT \u2014 Contextual Transformer Embeddings \u26a1 GPU\n    - 11.4 Embedding Method Comparison\n12. [Ensemble Learning \u2014 Soft Voting](#12)\n13. [Model Comparison & Selection](#13)\n14. [Feature Importance & Interpretability](#14)\n15. [Conclusions](#15)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s1",
   "metadata": {},
   "source": "---\n## 1. Setup & Imports <a id='1'></a>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport re\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# NLP\nimport nltk\nnltk.download('stopwords', quiet=True)\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n# Feature engineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n# Model selection & evaluation\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, roc_auc_score,\n    roc_curve, precision_recall_curve, auc,\n    precision_score, recall_score, f1_score\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\n\n# Class imbalance\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Persistence\nimport joblib\n\n# Visualisation\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom tqdm import tqdm\ntqdm.pandas()\n\n# Progress bar for pandas\nplt.rcParams.update({'figure.dpi': 120, 'font.size': 11})\nprint('All imports successful.')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s2",
   "metadata": {},
   "source": "---\n## 2. Data Loading & Exploratory Analysis <a id='2'></a>\n\nThe dataset is sourced from [Kaggle \u2014 Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews). Place `Reviews.csv` in the `../data/` directory before running this cell."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load",
   "metadata": {},
   "outputs": [],
   "source": "df = pd.read_csv('../data/Reviews.csv')\nprint(f'Shape: {df.shape}')\ndf.head(3)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eda-info",
   "metadata": {},
   "outputs": [],
   "source": "# Rename for clarity\ndf.columns = ['Id','ProductId','UserId','ProfileName',\n              'VotesHelpful','VotesTotal','Score','Time','Summary','Text']\n\nprint('Missing values:')\nprint(df[['Score','Summary','Text']].isnull().sum())\nprint(f'\\nUnique products : {df.ProductId.nunique():,}')\nprint(f'Unique users    : {df.UserId.nunique():,}')\nprint(f'Score range     : {df.Score.min()} \u2013 {df.Score.max()}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-score-dist",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n\n# Score distribution\nax = axes[0]\nscore_counts = df['Score'].value_counts().sort_index()\ncolors = ['#e74c3c','#e67e22','#f39c12','#27ae60','#2ecc71']\nax.bar(score_counts.index, score_counts.values, color=colors, edgecolor='white', linewidth=0.8)\nax.set_xlabel('Star Rating')\nax.set_ylabel('Number of Reviews')\nax.set_title('Score Distribution (raw data)')\nfor i, (x, y) in enumerate(zip(score_counts.index, score_counts.values)):\n    ax.text(x, y + 2000, f'{y:,}', ha='center', va='bottom', fontsize=9)\n\n# Text length distribution\nax2 = axes[1]\ndf['text_len'] = df['Text'].str.split().str.len()\nfor score, color, label in [(5,'#2ecc71','5-star'), (1,'#e74c3c','1-star')]:\n    subset = df[df['Score'] == score]['text_len'].clip(0, 500)\n    ax2.hist(subset, bins=50, alpha=0.55, color=color, label=label, density=True)\nax2.set_xlabel('Review length (words, clipped at 500)')\nax2.set_ylabel('Density')\nax2.set_title('Review Length Distribution by Extreme Scores')\nax2.legend()\n\nplt.tight_layout()\nplt.show()\nprint(f'Median review length: {df.text_len.median():.0f} words')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-label-md",
   "metadata": {},
   "source": "### 2.1 Label Construction\n\nWe convert the 5-point Likert scale to a binary sentiment label:\n\n$$\ny_i = \\begin{cases} 1 \\;(\\text{positive}) & \\text{if } s_i > 3 \\\\ 0 \\;(\\text{negative}) & \\text{if } s_i < 3 \\\\ \\text{excluded} & \\text{if } s_i = 3 \\end{cases}\n$$\n\nReviews with $s_i = 3$ represent ambiguous/neutral opinions and are excluded to prevent label noise."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-label",
   "metadata": {},
   "outputs": [],
   "source": "# Assign binary labels; drop Score == 3\ndf = df[df['Score'] != 3].copy()\ndf['Sentiment'] = (df['Score'] > 3).astype(int)  # 1=positive, 0=negative\n\ncounts = df['Sentiment'].value_counts()\nprint('Class distribution after exclusion:')\nprint(f'  Positive (1): {counts[1]:>7,}  ({counts[1]/len(df)*100:.1f}%)')\nprint(f'  Negative (0): {counts[0]:>7,}  ({counts[0]/len(df)*100:.1f}%)')\nprint(f'  Total       : {len(df):>7,}')\n\nfig, ax = plt.subplots(figsize=(5, 3.5))\nax.barh(['Negative (0)','Positive (1)'], [counts[0], counts[1]],\n        color=['#e74c3c','#2ecc71'], edgecolor='white')\nfor i, v in enumerate([counts[0], counts[1]]):\n    ax.text(v + 2000, i, f'{v:,} ({v/len(df)*100:.1f}%)', va='center')\nax.set_xlabel('Count')\nax.set_title('Binary Label Distribution')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s3",
   "metadata": {},
   "source": "---\n## 3. Text Preprocessing <a id='3'></a>\n\n### 3.1 Why We Keep Negation Stop Words\n\nStandard NLP pipelines routinely remove **stop words** (high-frequency, low-information words like \"the\", \"a\", \"is\"). However, **sentiment analysis is a notable exception**.\n\nNegation words such as *not*, *won't*, *can't*, *never*, *didn't* are typically included in stop-word lists, yet they are semantically critical:\n\n| Original phrase | After stop-word removal | Meaning preserved? |\n|---|---|---|\n| \"not great\" | \"great\" | \u274c Inverted! |\n| \"won't buy again\" | \"buy\" | \u274c Lost negative signal |\n| \"can't recommend\" | \"recommend\" | \u274c Inverted! |\n| \"absolutely delicious\" | \"absolutely delicious\" | \u2705 OK |\n\n**Conclusion**: We apply a *custom* stop-word list that **excludes all negation and contraction words**. For the remaining stop words (articles, prepositions), removing them is safe.\n\n### 3.2 Stemming\n\nStemming reduces inflected forms to a common root using the Snowball algorithm:\n\n$$\n\\text{\"tasty\"} \\rightarrow \\text{\"tasti\"}, \\quad \\text{\"disappointment\"} \\rightarrow \\text{\"disappoint\"}, \\quad \\text{\"loved\"} \\rightarrow \\text{\"love\"}\n$$\n\nThis reduces vocabulary size and helps the model generalise across morphological variants, without losing polarity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stopwords",
   "metadata": {},
   "outputs": [],
   "source": "# --- Build custom stop-word list that PRESERVES negation ---\nall_stop = set(stopwords.words('english'))\n\n# Negation / contraction words to KEEP\nnegation_words = {\n    'not', 'no', 'nor', 'never', 'neither', 'nobody', 'nothing', 'nowhere',\n    \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\",\n    \"can't\", \"cannot\", \"couldn't\", \"shouldn't\",\n    \"isn't\", \"aren't\", \"wasn't\", \"weren't\",\n    \"haven't\", \"hasn't\", \"hadn't\", \"ain't\",\n    \"mightn't\", \"mustn't\", \"needn't\"\n}\n\ncustom_stop = all_stop - negation_words  # remove negation from stop-list\n\nprint(f'Original stop-word list : {len(all_stop)} words')\nprint(f'Negation words preserved: {len(negation_words)}')\nprint(f'Final stop-word list    : {len(custom_stop)} words')\nprint(f'\\nSample negation words kept: {sorted(list(negation_words))[:8]}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cleanfunc",
   "metadata": {},
   "outputs": [],
   "source": "stemmer = SnowballStemmer('english')\n\ndef clean_text(text: str) -> str:\n    \"\"\"Lowercase \u2192 remove punctuation \u2192 remove custom stop words \u2192 stem.\"\"\"\n    text = str(text).lower()\n    # Replace punctuation with space (keep apostrophes for contractions)\n    text = re.sub(r'[?!.,;:)(|/]', ' ', text)\n    tokens = text.split()\n    cleaned = [\n        stemmer.stem(tok)\n        for tok in tokens\n        if tok not in custom_stop\n    ]\n    # Remove leftover special chars\n    out = ' '.join(cleaned)\n    out = re.sub(r\"[\\\"#]\", '', out)\n    return out\n\n# Quick sanity check\ntest_phrases = [\n    \"Not as advertised \u2014 absolutely terrible!\",\n    \"I won't buy this again. Disappointing.\",\n    \"Perfect! Highly recommend to everyone.\",\n]\nfor p in test_phrases:\n    print(f'  Original : {p}')\n    print(f'  Cleaned  : {clean_text(p)}')\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-apply-clean",
   "metadata": {},
   "outputs": [],
   "source": "# Apply to the Text column (full review body)\ndf = df[['Score', 'Sentiment', 'Summary', 'Text']].copy()\ndf['CleanText'] = df['Text'].progress_apply(clean_text)\nprint('Cleaning complete.')\ndf[['Text','CleanText']].head(3)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-wc-md",
   "metadata": {},
   "source": "### 3.3 Word Cloud Visualisation\n\nWord clouds give an intuitive view of the most frequent tokens in each sentiment class. They help validate that preprocessing preserved meaningful content."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-wordcloud",
   "metadata": {},
   "outputs": [],
   "source": "def make_wordcloud(text_series, title, ax, colormap='Greens'):\n    wc = WordCloud(\n        background_color='white',\n        max_words=150,\n        max_font_size=60,\n        colormap=colormap,\n        width=600, height=350,\n        random_state=42\n    ).generate(' '.join(text_series.dropna().values))\n    ax.imshow(wc, interpolation='bilinear')\n    ax.axis('off')\n    ax.set_title(title, fontsize=13, pad=10)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\nmake_wordcloud(df.loc[df.Sentiment==1, 'CleanText'], 'Positive Reviews (Score > 3)', axes[0], 'Greens')\nmake_wordcloud(df.loc[df.Sentiment==0, 'CleanText'], 'Negative Reviews (Score < 3)', axes[1], 'Reds')\nplt.suptitle('Word Clouds after Preprocessing', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s4",
   "metadata": {},
   "source": "---\n## 4. Feature Engineering \u2014 TF-IDF <a id='4'></a>\n\n### 4.1 Term Frequency\u2013Inverse Document Frequency\n\nTF-IDF is a numerical statistic that reflects how important a word is to a document relative to the corpus. For a term $t$ in document $d$ within corpus $D$:\n\n$$\n\\text{TF}(t,d) = \\frac{\\text{count}(t,d)}{\\sum_{t'} \\text{count}(t',d)}\n$$\n\n$$\n\\text{IDF}(t,D) = \\log\\!\\left(\\frac{|D| + 1}{|\\{d \\in D : t \\in d\\}| + 1}\\right) + 1\n$$\n\n$$\n\\text{TF-IDF}(t,d,D) = \\text{TF}(t,d) \\times \\text{IDF}(t,D)\n$$\n\n- **High TF, high IDF** \u2192 word is frequent in this document but rare corpus-wide \u2192 very distinctive, high weight.\n- **High TF, low IDF** \u2192 word is frequent everywhere (e.g., \"the\") \u2192 low weight.\n\n### 4.2 N-gram Range\n\nWe use **unigrams + bigrams** (`ngram_range=(1,2)`), capturing both individual words and two-word phrases:\n\n| Unigrams | Bigrams |\n|---|---|\n| \"not\" | \"not great\" |\n| \"recommend\" | \"highly recommend\" |\n| \"disappoint\" | \"won't buy\" |\n\nThis is important because bigrams preserve negation context that unigrams lose."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-tfidf",
   "metadata": {},
   "outputs": [],
   "source": "vectorizer = TfidfVectorizer(\n    ngram_range=(1, 2),\n    max_features=10_000,\n    min_df=5,           # ignore very rare terms\n    sublinear_tf=True   # apply log(1+tf) to dampen extremely frequent terms\n)\n\nX = vectorizer.fit_transform(df['CleanText'])\ny = df['Sentiment'].values\n\nprint(f'TF-IDF matrix shape : {X.shape}')\nprint(f'Matrix density      : {X.nnz / (X.shape[0]*X.shape[1]):.4%}')\nprint(f'Vocabulary size     : {len(vectorizer.vocabulary_):,}')\nprint(f'Sample features     : {list(vectorizer.get_feature_names_out()[:6])} ... {list(vectorizer.get_feature_names_out()[-6:])}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s5",
   "metadata": {},
   "source": "---\n## 5. Train / Test Split <a id='5'></a>\n\n**Critical rule**: The test set is held out immediately after the split. All preprocessing fitted to data (SVD, SMOTE, undersampling, cross-validation) is applied **only to the training partition** to prevent information leakage."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-split",
   "metadata": {},
   "outputs": [],
   "source": "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\nprint(f'Training set : {X_train.shape[0]:,} samples  '\n      f'(pos={y_train.sum():,}, neg={(y_train==0).sum():,})')\nprint(f'Test set     : {X_test.shape[0]:,} samples  '\n      f'(pos={y_test.sum():,}, neg={(y_test==0).sum():,})')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s6",
   "metadata": {},
   "source": "---\n## 6. Baseline Model \u2014 Logistic Regression <a id='6'></a>\n\n### Why Baseline First?\n\nA baseline model trains with **no special handling** of class imbalance, no regularisation tuning, and no dimensionality reduction. It establishes the reference point against which every improvement is measured.\n\n### Logistic Regression Review\n\nFor a binary target $y \\in \\{0,1\\}$, Logistic Regression models the conditional probability:\n\n$$\nP(y=1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^\\top \\mathbf{x} + b)}}\n$$\n\nParameters $\\mathbf{w}$ and $b$ are estimated by maximising the log-likelihood (equivalently, minimising cross-entropy loss):\n\n$$\n\\mathcal{L}(\\mathbf{w}) = -\\frac{1}{N}\\sum_{i=1}^{N}\\left[y_i \\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i)\\right]\n$$\n\nThe prediction threshold is $\\hat{y} = 1$ if $\\hat{p} \\geq 0.5$, else $\\hat{y} = 0$."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-baseline",
   "metadata": {},
   "outputs": [],
   "source": "lr_baseline = LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs')\nlr_baseline.fit(X_train, y_train)\n\ny_pred_base = lr_baseline.predict(X_test)\ny_prob_base = lr_baseline.predict_proba(X_test)[:, 1]\n\nprint('=== Baseline Logistic Regression ===')\nprint(classification_report(y_test, y_pred_base, target_names=['Negative','Positive'], digits=4))\nprint(f'AUC-ROC: {roc_auc_score(y_test, y_prob_base):.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-baseline-cm",
   "metadata": {},
   "outputs": [],
   "source": "def plot_evaluation(y_true, y_prob, y_pred, title, ax_cm, ax_roc):\n    \"\"\"Plot confusion matrix and ROC curve side by side.\"\"\"\n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax_cm,\n                xticklabels=['Pred Neg','Pred Pos'],\n                yticklabels=['True Neg','True Pos'])\n    neg_recall = cm[0,0] / cm[0].sum()\n    pos_recall = cm[1,1] / cm[1].sum()\n    ax_cm.set_title(f'{title}\\nNeg Recall={neg_recall:.3f} | Pos Recall={pos_recall:.3f}')\n\n    # ROC curve\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    roc_auc = auc(fpr, tpr)\n    ax_roc.plot(fpr, tpr, lw=2, label=f'AUC = {roc_auc:.4f}')\n    ax_roc.plot([0,1],[0,1],'k--', lw=1)\n    ax_roc.set_xlabel('False Positive Rate')\n    ax_roc.set_ylabel('True Positive Rate')\n    ax_roc.set_title(f'ROC Curve \u2014 {title}')\n    ax_roc.legend(loc='lower right')\n    ax_roc.grid(alpha=0.3)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\nplot_evaluation(y_test, y_prob_base, y_pred_base, 'Baseline LR', axes[0], axes[1])\nplt.tight_layout()\nplt.show()\n\nprint('Observation: Baseline achieves ~90% accuracy, but negative-class recall')\nprint('is only ~0.69 \u2014 31% of dissatisfied customers are MISSED.')\nprint('This is unacceptable for the business use case.')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s7",
   "metadata": {},
   "source": "---\n## 7. Class Imbalance: Problem & Solutions <a id='7'></a>\n\nWith **78% positive / 22% negative**, a naive model can achieve 78% accuracy by predicting \"positive\" for everything. We need strategies to improve minority-class detection.\n\n### Business Rationale for Prioritising Recall\n\n| Error Type | Business Impact |\n|---|---|\n| False Negative (miss a real negative) | Customer complaint unaddressed \u2192 churn, reputation damage |\n| False Positive (flag a real positive as negative) | Unnecessary follow-up \u2192 minor cost |\n\nConclusion: **Maximise negative-class Recall** (subject to a minimum Precision constraint).\n\n### Evaluation Metrics\n\n$$\n\\text{Precision} = \\frac{TP}{TP + FP}, \\quad \\text{Recall} = \\frac{TP}{TP + FN}\n$$\n\n$$\nF_1 = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$$\n\n$$\n\\text{AUC-ROC} = \\int_0^1 \\text{TPR}\\,d(\\text{FPR})\n$$\n\nAll metrics reported are for the **negative class (label = 0)** unless otherwise stated."
  },
  {
   "cell_type": "markdown",
   "id": "cell-s7-1",
   "metadata": {},
   "source": "### 7.1 Strategy 1 \u2014 Class Weights\n\nThe most computationally efficient approach: the loss function is re-weighted inversely proportional to class frequency.\n\n$$\nw_c = \\frac{N}{K \\cdot N_c}\n$$\n\nwhere $N$ = total samples, $K$ = number of classes, $N_c$ = samples in class $c$.\n\n- **Advantages**: No data modification; no information loss; no extra computation.\n- **Disadvantages**: Only adjusts the decision boundary \u2014 does not augment or remove data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-class-weights",
   "metadata": {},
   "outputs": [],
   "source": "lr_cw = LogisticRegression(max_iter=1000, random_state=42,\n                            class_weight='balanced', solver='lbfgs')\nlr_cw.fit(X_train, y_train)\n\ny_pred_cw = lr_cw.predict(X_test)\ny_prob_cw = lr_cw.predict_proba(X_test)[:, 1]\n\nprint('=== LR + Class Weights ===')\nprint(classification_report(y_test, y_pred_cw, target_names=['Negative','Positive'], digits=4))\nprint(f'AUC-ROC: {roc_auc_score(y_test, y_prob_cw):.4f}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s7-svd",
   "metadata": {},
   "source": "### 7.2 Strategy 2 \u2014 SMOTE (with SVD Preprocessing)\n\n**SMOTE** (Synthetic Minority Over-sampling Technique) generates synthetic minority-class samples by linear interpolation between existing samples in feature space:\n\n$$\n\\mathbf{x}_{\\text{new}} = \\mathbf{x}_i + \\lambda \\cdot (\\mathbf{x}_{\\text{nn}} - \\mathbf{x}_i), \\quad \\lambda \\sim \\text{Uniform}(0,1)\n$$\n\nwhere $\\mathbf{x}_{\\text{nn}}$ is one of the $k$ nearest neighbours of $\\mathbf{x}_i$ in feature space.\n\n**Why SVD is needed here (and only here)**:  \nSMOTE requires a **dense** feature matrix. The TF-IDF output is sparse (10,000 dimensions). We apply Truncated SVD to compress it to a dense representation \u2014 **but only as a preprocessing step for SMOTE, not as a primary feature-extraction strategy**.\n\n**Critical limitation of SVD in this context**:  \nAs shown in the plot below, 1,000 SVD components explain < 70% of the total variance. This is far below the typical 90%+ threshold used to justify dimensionality reduction. The text features are distributed across many dimensions and do not compress well. In contrast to tabular data, NLP TF-IDF matrices often require thousands of components to preserve most information."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-svd-analysis",
   "metadata": {},
   "outputs": [],
   "source": "# --- SVD Explained Variance Analysis ---\n# Fit SVD on training data only\nsvd = TruncatedSVD(n_components=1000, random_state=42)\nsvd.fit(X_train)\n\ncumvar = np.cumsum(svd.explained_variance_ratio_)\ncomponents = np.arange(1, 1001)\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\n\n# Full curve\naxes[0].plot(components, cumvar, color='steelblue', lw=1.5)\naxes[0].axhline(0.90, color='red', ls='--', lw=1.2, label='90% threshold')\naxes[0].axhline(0.70, color='orange', ls='--', lw=1.2, label='70% threshold')\naxes[0].fill_between(components, cumvar, alpha=0.15, color='steelblue')\naxes[0].set_xlabel('Number of Components')\naxes[0].set_ylabel('Cumulative Explained Variance')\naxes[0].set_title('Truncated SVD \u2014 Explained Variance (1\u20131000 components)')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\naxes[0].set_ylim(0, 1.02)\n\n# Marginal gain\naxes[1].plot(components, svd.explained_variance_ratio_, color='darkorange', lw=1.2)\naxes[1].set_xlabel('Component index')\naxes[1].set_ylabel('Individual Explained Variance')\naxes[1].set_title('Marginal Explained Variance per Component')\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Find how many components for 90%\nthresh_90 = np.searchsorted(cumvar, 0.90)\nprint(f'Variance explained by 1000 components : {cumvar[-1]:.2%}')\nprint(f'Components needed for 90% variance    : {thresh_90 if thresh_90 < 1000 else \">1000\"}')\nprint()\nprint('Interpretation: SVD is INEFFECTIVE as primary dimensionality reduction')\nprint('for TF-IDF NLP features. Use it only when dense input is required (e.g. SMOTE).')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-smote",
   "metadata": {},
   "outputs": [],
   "source": "# Apply SVD transformation (dense input needed for SMOTE)\nX_train_svd = svd.transform(X_train)   # shape (N_train, 1000)\nX_test_svd  = svd.transform(X_test)    # shape (N_test,  1000)\n\n# Apply SMOTE to the SVD-reduced training data\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train_svd, y_train)\nprint(f'After SMOTE: {X_train_smote.shape[0]:,} training samples '\n      f'(pos={y_train_smote.sum():,}, neg={(y_train_smote==0).sum():,})')\n\nlr_smote = LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs')\nlr_smote.fit(X_train_smote, y_train_smote)\n\ny_pred_smote = lr_smote.predict(X_test_svd)\ny_prob_smote = lr_smote.predict_proba(X_test_svd)[:, 1]\n\nprint('\\n=== LR + SMOTE (SVD-reduced features) ===')\nprint(classification_report(y_test, y_pred_smote, target_names=['Negative','Positive'], digits=4))\nprint(f'AUC-ROC: {roc_auc_score(y_test, y_prob_smote):.4f}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s7-3",
   "metadata": {},
   "source": "### 7.3 Strategy 3 \u2014 Random Undersampling\n\nRandom undersampling randomly removes majority-class training samples until both classes are equal in size.\n\n$$\nN'_{\\text{majority}} = N_{\\text{minority}}\n$$\n\n- **Advantages**: Fast; eliminates imbalance entirely; works on sparse matrices (unlike SMOTE).\n- **Disadvantages**: Discards potentially useful majority-class information (information loss)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-undersampling",
   "metadata": {},
   "outputs": [],
   "source": "rus = RandomUnderSampler(random_state=42)\nX_rus, y_rus = rus.fit_resample(X_train, y_train)\nprint(f'After undersampling: {X_rus.shape[0]:,} training samples '\n      f'(pos={y_rus.sum():,}, neg={(y_rus==0).sum():,})')\n\nlr_rus = LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs')\nlr_rus.fit(X_rus, y_rus)\n\ny_pred_rus = lr_rus.predict(X_test)\ny_prob_rus = lr_rus.predict_proba(X_test)[:, 1]\n\nprint('\\n=== LR + Random Undersampling ===')\nprint(classification_report(y_test, y_pred_rus, target_names=['Negative','Positive'], digits=4))\nprint(f'AUC-ROC: {roc_auc_score(y_test, y_prob_rus):.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-strategy-compare",
   "metadata": {},
   "outputs": [],
   "source": "# Summary table of class-imbalance strategies for LR\nresults_imbalance = {\n    'Baseline (no balancing)':   (y_test, y_pred_base, y_prob_base),\n    'Class Weights':             (y_test, y_pred_cw,   y_prob_cw),\n    'SMOTE + SVD':               (y_test, y_pred_smote, y_prob_smote),\n    'Random Undersampling':      (y_test, y_pred_rus,  y_prob_rus),\n}\n\nrows = []\nfor name, (yt, yp, ypr) in results_imbalance.items():\n    rows.append({\n        'Strategy': name,\n        'Accuracy': f\"{(yt==yp).mean():.4f}\",\n        'Neg Recall': f\"{recall_score(yt, yp, pos_label=0):.4f}\",\n        'Neg Precision': f\"{precision_score(yt, yp, pos_label=0):.4f}\",\n        'Neg F1': f\"{f1_score(yt, yp, pos_label=0):.4f}\",\n        'AUC': f\"{roc_auc_score(yt, ypr):.4f}\",\n    })\n\ndf_imb = pd.DataFrame(rows)\nprint(df_imb.to_string(index=False))\nprint('\\nTakeaway: Class Weights best balances recall improvement and precision retention.')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s8",
   "metadata": {},
   "source": "---\n## 8. Regularisation & Hyperparameter Tuning <a id='8'></a>\n\n### ElasticNet Regularisation\n\nL2 (Ridge) regularisation shrinks coefficients toward zero but keeps all features. L1 (Lasso) drives some coefficients to exactly zero (sparse solutions). **ElasticNet** interpolates between them:\n\n$$\n\\mathcal{L}_{\\text{EN}}(\\mathbf{w}) = \\mathcal{L}_{\\text{CE}}(\\mathbf{w}) + \\lambda \\Big[\\alpha \\|\\mathbf{w}\\|_1 + (1-\\alpha)\\|\\mathbf{w}\\|_2^2 \\Big]\n$$\n\nwhere $\\alpha$ (the `l1_ratio` parameter) controls the mix. We tune $\\alpha$ via **3-fold cross-validation**, optimising for negative-class Recall."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gridsearch",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import make_scorer\n\nrecall_neg = make_scorer(recall_score, pos_label=0)\n\nparam_grid = {'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]}\nbase_lr = LogisticRegression(\n    penalty='elasticnet', solver='saga',\n    class_weight='balanced',\n    max_iter=1000, random_state=42\n)\n\ngrid = GridSearchCV(\n    base_lr, param_grid,\n    scoring=recall_neg, cv=3, n_jobs=-1, verbose=1,\n    return_train_score=True\n)\ngrid.fit(X_train, y_train)\n\nprint(f'Best l1_ratio : {grid.best_params_[\"l1_ratio\"]}')\nprint(f'Best CV Recall (neg): {grid.best_score_:.4f}')\n\n# Plot CV results\ncv_results = pd.DataFrame(grid.cv_results_)\nfig, ax = plt.subplots(figsize=(7, 4))\nax.plot(param_grid['l1_ratio'], cv_results['mean_test_score'], 'o-', color='steelblue', lw=2)\nax.fill_between(param_grid['l1_ratio'],\n                cv_results['mean_test_score'] - cv_results['std_test_score'],\n                cv_results['mean_test_score'] + cv_results['std_test_score'],\n                alpha=0.2, color='steelblue')\nax.set_xlabel('l1_ratio (0 = pure L2, 1 = pure L1)')\nax.set_ylabel('Mean CV Recall (negative class)')\nax.set_title('GridSearchCV \u2014 ElasticNet l1_ratio Tuning')\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-best-lr",
   "metadata": {},
   "outputs": [],
   "source": "best_lr = grid.best_estimator_\ny_pred_best_lr = best_lr.predict(X_test)\ny_prob_best_lr = best_lr.predict_proba(X_test)[:, 1]\n\nprint('=== Best LR: Balanced + ElasticNet ===')\nprint(classification_report(y_test, y_pred_best_lr, target_names=['Negative','Positive'], digits=4))\nprint(f'AUC-ROC: {roc_auc_score(y_test, y_prob_best_lr):.4f}')\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\nplot_evaluation(y_test, y_prob_best_lr, y_pred_best_lr,\n                'Best LR (Balanced + ElasticNet)', axes[0], axes[1])\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s9",
   "metadata": {},
   "source": "---\n## 9. Threshold Tuning \u2014 Balancing Precision & Recall <a id='9'></a>\n\nThe default decision threshold is $\\tau = 0.5$: predict negative if $\\hat{p} < 0.5$. But this is arbitrary. Lowering $\\tau$ increases recall (catches more negatives) at the cost of precision (more false alarms). We can find a threshold that **balances both** according to business requirements.\n\n### Precision\u2013Recall Curve\n\nFor a continuous range of thresholds $\\tau \\in [0,1]$:\n\n$$\n\\text{Precision}(\\tau), \\quad \\text{Recall}(\\tau), \\quad \\text{F}_\\beta(\\tau) = (1+\\beta^2)\\cdot\\frac{\\text{Prec} \\cdot \\text{Rec}}{\\beta^2 \\cdot \\text{Prec} + \\text{Rec}}\n$$\n\nWe plot the PR curve and identify the **threshold that maximises F1** for the negative class, as well as the threshold where precision $\\geq 0.72$ (a business floor).\n\n**This addresses the TA feedback**: instead of blindly lowering the threshold to maximise recall at the expense of precision, we find a principled trade-off point."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-threshold",
   "metadata": {},
   "outputs": [],
   "source": "# Precision-Recall curve for the NEGATIVE class\n# Note: sklearn uses pos_label for the 'positive' class in the PR curve.\n# We set pos_label=0 to treat negative as the positive of interest.\nprecisions, recalls, thresholds = precision_recall_curve(\n    y_test, 1 - y_prob_best_lr,  # invert prob to make neg the 'positive'\n    pos_label=1\n)\n# thresholds has one fewer element\nf1_scores = 2 * precisions[:-1] * recalls[:-1] / (precisions[:-1] + recalls[:-1] + 1e-8)\n\n# Find best F1 threshold\nbest_f1_idx = np.argmax(f1_scores)\nbest_threshold = 1 - thresholds[best_f1_idx]   # convert back to original prob space\nprint(f'Threshold for max negative-class F1    : {best_threshold:.3f}')\nprint(f'  Precision @ this threshold: {precisions[best_f1_idx]:.4f}')\nprint(f'  Recall    @ this threshold: {recalls[best_f1_idx]:.4f}')\nprint(f'  F1        @ this threshold: {f1_scores[best_f1_idx]:.4f}')\n\n# Find threshold where precision >= 0.72\nprec_floor = 0.72\nvalid = np.where(precisions[:-1] >= prec_floor)[0]\nif len(valid) > 0:\n    prec_thresh_idx = valid[np.argmax(recalls[:-1][valid])]\n    prec_thresh = 1 - thresholds[prec_thresh_idx]\n    print(f'\\nThreshold for Precision \u2265 {prec_floor} (max recall): {prec_thresh:.3f}')\n    print(f'  Precision @ this threshold: {precisions[prec_thresh_idx]:.4f}')\n    print(f'  Recall    @ this threshold: {recalls[prec_thresh_idx]:.4f}')\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\n\n# PR Curve\naxes[0].plot(recalls[:-1], precisions[:-1], color='steelblue', lw=1.8, label='PR curve')\naxes[0].scatter(recalls[best_f1_idx], precisions[best_f1_idx],\n                color='red', zorder=5, s=80, label=f'Max F1 (\u03c4={best_threshold:.2f})')\nif len(valid) > 0:\n    axes[0].scatter(recalls[prec_thresh_idx], precisions[prec_thresh_idx],\n                    color='green', zorder=5, s=80, marker='s',\n                    label=f'Prec\u22650.72 (\u03c4={prec_thresh:.2f})')\naxes[0].axhline(prec_floor, color='green', ls='--', lw=1, alpha=0.7)\naxes[0].set_xlabel('Recall (Negative class)')\naxes[0].set_ylabel('Precision (Negative class)')\naxes[0].set_title('Precision\u2013Recall Curve (Negative class)')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# F1 vs threshold\naxes[1].plot(1 - thresholds, f1_scores, color='darkorange', lw=1.8)\naxes[1].axvline(best_threshold, color='red', ls='--', lw=1.2, label=f'Max F1 at \u03c4={best_threshold:.2f}')\naxes[1].set_xlabel('Classification Threshold (for negative class)')\naxes[1].set_ylabel('F1 Score (Negative class)')\naxes[1].set_title('F1 Score vs Threshold')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-threshold-apply",
   "metadata": {},
   "outputs": [],
   "source": "# Apply custom threshold (max F1)\ny_pred_tuned = (y_prob_best_lr < best_threshold).astype(int)\nprint(f'=== Best LR + Threshold Tuning (\u03c4={best_threshold:.3f}) ===')\nprint(classification_report(y_test, y_pred_tuned, target_names=['Negative','Positive'], digits=4))\nprint(f'AUC-ROC: {roc_auc_score(y_test, y_prob_best_lr):.4f}  (unchanged \u2014 AUC is threshold-independent)')\nprint()\nprint('Threshold tuning improves the Precision\u2013Recall balance')\nprint('without retraining. This is the recommended approach when')\nprint('the deployment requirements change.')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s10",
   "metadata": {},
   "source": "---\n## 10. Random Forest Classifier <a id='10'></a>\n\nRandom Forest is a **bagging ensemble** of decision trees. Each tree is trained on a bootstrap sample, and at each split, only a random subset of $m \\approx \\sqrt{p}$ features is considered:\n\n$$\n\\hat{p}(c \\mid \\mathbf{x}) = \\frac{1}{T}\\sum_{t=1}^{T} \\hat{p}_t(c \\mid \\mathbf{x})\n$$\n\nAdvantages over Logistic Regression:\n- Captures non-linear feature interactions\n- Inherently resistant to overfitting via bagging\n- Provides feature importance via mean decrease in impurity\n\nDisadvantages:\n- Cannot operate directly on sparse TF-IDF without SVD (memory / speed)\n- Less interpretable coefficients"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-rf",
   "metadata": {},
   "outputs": [],
   "source": "# RF trained on SVD-reduced features (dense; avoids memory issues)\n# We use the best params found previously via GridSearchCV to save time\nbest_rf = RandomForestClassifier(\n    n_estimators=500,\n    max_depth=20,\n    max_features='sqrt',\n    min_samples_split=5,\n    min_samples_leaf=5,\n    class_weight='balanced',\n    random_state=42,\n    n_jobs=-1\n)\nbest_rf.fit(X_train_svd, y_train)\n\ny_pred_rf = best_rf.predict(X_test_svd)\ny_prob_rf  = best_rf.predict_proba(X_test_svd)[:, 1]\n\nprint('=== Random Forest (Balanced, SVD-reduced features) ===')\nprint(classification_report(y_test, y_pred_rf, target_names=['Negative','Positive'], digits=4))\nprint(f'AUC-ROC: {roc_auc_score(y_test, y_prob_rf):.4f}')\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\nplot_evaluation(y_test, y_prob_rf, y_pred_rf, 'Random Forest (Balanced+SVD)', axes[0], axes[1])\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-rf-importance",
   "metadata": {},
   "outputs": [],
   "source": "# Feature importance from Random Forest (mapped back to SVD components)\n# Since SVD mixes features, we show the top TF-IDF importance through LR for interpretability\n# Here we show the RF's SVD-component importances\nimportances_rf = best_rf.feature_importances_\ntop_k = 30\ntop_idx = np.argsort(importances_rf)[::-1][:top_k]\n\nfig, ax = plt.subplots(figsize=(9, 7))\nax.barh(range(top_k), importances_rf[top_idx][::-1], color='steelblue', edgecolor='none')\nax.set_yticks(range(top_k))\nax.set_yticklabels([f'SVD Component {i+1}' for i in top_idx[::-1]], fontsize=8)\nax.set_xlabel('Feature Importance (Mean Decrease in Impurity)')\nax.set_title(f'Top {top_k} Random Forest Feature Importances (SVD components)')\nax.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.show()\nprint('Note: RF importances are over SVD components, not original words.')\nprint('For word-level importance, see Section 14 (Logistic Regression coefficients).')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s11",
   "metadata": {},
   "source": "---\n## 11. Beyond Bag-of-Words: Word Embeddings <a id='11'></a>\n\n### 11.1 Theory and Motivation\n\n**TF-IDF** treats each word as an independent feature with no notion of semantic similarity. The words \"terrible\" and \"awful\" have orthogonal TF-IDF representations despite being near-synonyms. Bigrams capture some local context, but meaning is still derived from frequency patterns, not semantics.\n\n**Word Embeddings** address this by mapping each word to a dense vector in a continuous semantic space, so that semantically similar words are close:\n\n$$\n\\cos(\\mathbf{e}_{\\text{terrible}},\\, \\mathbf{e}_{\\text{awful}}) \\approx 1, \\qquad \\cos(\\mathbf{e}_{\\text{terrible}},\\, \\mathbf{e}_{\\text{delicious}}) \\approx 0\n$$\n\n### Representation Methods Compared\n\n| Method | Type | Semantic Similarity | Context-Aware | Domain |\n|---|---|---|---|---|\n| TF-IDF | Sparse, count-based | \u274c None | \u274c | Fit to corpus |\n| Word2Vec (avg. pool) | Dense, static | \u2705 Yes | \u274c Order-blind | Fit to corpus |\n| DistilBERT (fine-tuned) | Dense, contextual | \u2705 Yes | \u2705 Full context | Pre-trained + adapted |\n\n### Key Limitation of Average Pooling\n\nWord2Vec + average pooling still loses **word order**. The phrases \"not great\" and \"not terrible\" will have similar document vectors if the component words have similar embeddings \u2014 exactly the opposite of what we want. This is the core weakness that contextual models overcome:\n\n$$\n\\mathbf{v}_d^{\\text{W2V}} = \\frac{1}{n}\\sum_{i} \\mathbf{e}_{w_i} \\qquad \\text{(order-blind)}\n$$\n\n$$\n\\mathbf{v}_d^{\\text{BERT}} = \\text{Transformer}(w_1, w_2, \\ldots, w_n) \\qquad \\text{(full context)}\n$$\n\nThe DistilBERT encoder processes the full sequence at once via **multi-head self-attention**:\n\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n$$\n\nallowing \"not\" in \"not great\" to modulate the representation of \"great\" based on its position and context."
  },
  {
   "cell_type": "code",
   "id": "blajq8xo8hc",
   "source": "# \u2500\u2500 Section 11 Preamble: package installs + text splits \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Install packages not in base Colab/conda environments\nimport subprocess, sys\nfor pkg in ['gensim', 'transformers', 'accelerate']:\n    try:\n        __import__(pkg)\n    except ImportError:\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '-q'])\n\n# \u2500\u2500 Colab: mount Google Drive and (optionally) reload data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport os\nON_COLAB = 'google.colab' in sys.modules or os.path.exists('/content')\nif ON_COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    # Update this path if your Reviews.csv is stored elsewhere in Drive:\n    COLAB_DATA = '/content/drive/MyDrive/amazon_reviews/Reviews.csv'\n    if not os.path.exists(COLAB_DATA):\n        print(f'WARNING: {COLAB_DATA} not found.')\n        print('Please upload Reviews.csv to your Drive and update COLAB_DATA above.')\n    else:\n        print('Drive mounted. Run Sections 1-5 first to get df, X, y in memory.')\n\n# \u2500\u2500 Recover text arrays from the same train/test split \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# (X_train/X_test were split from X in Section 5; we need the original text)\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nclean_texts = df['CleanText'].values\nraw_texts   = df['Text'].fillna('').values  # for BERT (raw, unprocessed)\ny_all       = df['Sentiment'].values        # same as y used in Section 4-5\n\n# Reproduce the exact same split (same random_state + stratify gives same indices)\n_, _, texts_train, texts_test, raw_train, raw_test, _, _ = train_test_split(\n    X, clean_texts, raw_texts, y_all,\n    test_size=0.20, random_state=42, stratify=y_all\n)\n\nimport torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')\nprint(f'texts_train: {len(texts_train):,}  texts_test: {len(texts_test):,}')\nprint(f'raw_train  : {len(raw_train):,}  raw_test  : {len(raw_test):,}')\n\n# Stratified subsample for DistilBERT fine-tuning (Section 11.3)\nfrom sklearn.model_selection import train_test_split as tts\nraw_ft, _, y_ft, _ = tts(\n    raw_train, y_train, train_size=50_000 + 5_000,\n    random_state=42, stratify=y_train\n)\nraw_ft_train, raw_ft_val, y_ft_train, y_ft_val = tts(\n    raw_ft, y_ft, test_size=5_000, random_state=42, stratify=y_ft\n)\nprint(f'BERT fine-tune train: {len(y_ft_train):,}  val: {len(y_ft_val):,}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2bi3bijwy5v",
   "source": "### 11.2 Word2Vec \u2014 Static Dense Embeddings <a id='11-2'></a>\n\n**Word2Vec** (Mikolov et al., 2013) learns embedding vectors by training a shallow neural network to predict surrounding context words (**Skip-gram**) or predict a target from context (**CBOW**):\n\n$$\n\\text{Skip-gram objective:} \\quad \\mathcal{L} = -\\sum_{t}\\sum_{c \\in \\mathcal{C}(t)} \\log \\sigma(\\mathbf{e}_{w_t}^\\top \\mathbf{e}_{w_c}) - k\\,\\mathbb{E}_{w \\sim P_n}\\left[\\log \\sigma(-\\mathbf{e}_{w_t}^\\top \\mathbf{e}_w)\\right]\n$$\n\nwhere the second term is negative sampling with $k$ noise words drawn from $P_n \\propto f(w)^{3/4}$.\n\nWe train on the **training corpus only** (no test data leakage) with:\n- Embedding dimension $d = 300$, window size $w = 5$\n- Minimum count = 5 (ignores rare words)\n- 5 training epochs with negative sampling ($k=5$)\n\n**Document representation** \u2014 average pooling over token embeddings:\n\n$$\n\\mathbf{v}_d = \\frac{1}{|V_d|}\\sum_{w \\in V_d} \\mathbf{e}_w \\in \\mathbb{R}^{300}\n$$\n\nThis dense 300-dim vector replaces the 10,000-dim sparse TF-IDF vector. We then train LR and RF on top.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2rh0u52cozm",
   "source": "%%time\nimport time\nfrom gensim.models import Word2Vec\n\n# Tokenise cleaned text into lists of tokens for Word2Vec\ntrain_sentences = [t.split() for t in texts_train]\ntest_sentences  = [t.split() for t in texts_test]\n\n# Train Skip-gram Word2Vec on the training corpus only\nprint('Training Word2Vec Skip-gram (d=300, window=5, min_count=5, epochs=5)...')\nw2v_model = Word2Vec(\n    sentences=train_sentences,\n    vector_size=300,   # embedding dimension\n    window=5,          # context window\n    min_count=5,       # ignore rare words\n    sg=1,              # Skip-gram (sg=0 = CBOW)\n    workers=4,\n    epochs=5,\n    seed=42\n)\nprint(f'Vocabulary: {len(w2v_model.wv):,} words  |  Embedding shape: {w2v_model.wv.vectors.shape}')\n\n# Inspect semantic neighbourhood\nprint('\\nMost similar to \"delici\" (stemmed \"delicious\"):',\n      [w for w, _ in w2v_model.wv.most_similar('delici', topn=5)])\nprint('Most similar to \"disappoint\":',\n      [w for w, _ in w2v_model.wv.most_similar('disappoint', topn=5)])\n# Negation is in the vocabulary since we kept \"not\"\nif 'not' in w2v_model.wv:\n    print('Most similar to \"not\":',\n          [w for w, _ in w2v_model.wv.most_similar('not', topn=5)])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "lfu2hi7wfu",
   "source": "def doc_to_vec(tokens, model, dim=300):\n    \"\"\"Average pooling over Word2Vec embeddings for all known tokens.\"\"\"\n    vecs = [model.wv[t] for t in tokens if t in model.wv]\n    return np.mean(vecs, axis=0).astype(np.float32) if vecs else np.zeros(dim, np.float32)\n\nprint('Building document embedding vectors...')\nt0 = time.time()\nX_w2v_train = np.vstack([doc_to_vec(s, w2v_model) for s in train_sentences])\nX_w2v_test  = np.vstack([doc_to_vec(s, w2v_model) for s in test_sentences])\nprint(f'Done in {time.time()-t0:.0f}s')\nprint(f'X_w2v_train: {X_w2v_train.shape}  (dense float32)')\noov = (X_w2v_train.sum(axis=1) == 0).sum()\nprint(f'All-zero vectors (no vocab coverage): {oov} / {len(X_w2v_train)}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f6wabuh3eht",
   "source": "%%time\n# Logistic Regression on Word2Vec document embeddings\nlr_w2v = LogisticRegression(\n    max_iter=1000, random_state=42,\n    class_weight='balanced', solver='lbfgs'\n)\nlr_w2v.fit(X_w2v_train, y_train)\n\ny_pred_w2v_lr = lr_w2v.predict(X_w2v_test)\ny_prob_w2v_lr = lr_w2v.predict_proba(X_w2v_test)[:, 1]\n\nprint('=== Word2Vec (avg. pool) + LR (Balanced) ===')\nprint(classification_report(y_test, y_pred_w2v_lr, target_names=['Negative','Positive'], digits=4))\nprint(f'AUC-ROC: {roc_auc_score(y_test, y_prob_w2v_lr):.4f}')\n\n# Quick comparison with TF-IDF LR (already in memory)\nprint('\\n--- Comparison with TF-IDF ---')\nprint(f'  TF-IDF + LR  :  NegF1={f1_score(y_test, y_pred_best_lr, pos_label=0):.4f}  '\n      f'AUC={roc_auc_score(y_test, y_prob_best_lr):.4f}')\nprint(f'  Word2Vec + LR:  NegF1={f1_score(y_test, y_pred_w2v_lr, pos_label=0):.4f}  '\n      f'AUC={roc_auc_score(y_test, y_prob_w2v_lr):.4f}')\nprint('\\nNote: TF-IDF often outperforms simple average-pooling Word2Vec on tasks')\nprint('with strong lexical signals (e.g. star-count bigrams, domain vocabulary).')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a0xkxxjys4q",
   "source": "%%time\n# Word2Vec + Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_w2v = RandomForestClassifier(\n    n_estimators=300, max_depth=20, max_features='sqrt',\n    class_weight='balanced', random_state=42, n_jobs=-1\n)\nrf_w2v.fit(X_w2v_train, y_train)\n\ny_pred_w2v_rf = rf_w2v.predict(X_w2v_test)\ny_prob_w2v_rf = rf_w2v.predict_proba(X_w2v_test)[:, 1]\n\nprint('=== Word2Vec + Random Forest (Balanced) ===')\nprint(classification_report(y_test, y_pred_w2v_rf, target_names=['Negative','Positive'], digits=4))\nprint(f'AUC-ROC: {roc_auc_score(y_test, y_prob_w2v_rf):.4f}')\n\n# Side-by-side comparison: LR vs RF on W2V\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\nfor ax, (name, yp, ypr) in zip(axes, [\n    ('Word2Vec + LR', y_pred_w2v_lr, y_prob_w2v_lr),\n    ('Word2Vec + RF', y_pred_w2v_rf, y_prob_w2v_rf),\n]):\n    cm = confusion_matrix(y_test, yp)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', ax=ax,\n                xticklabels=['Pred Neg','Pred Pos'],\n                yticklabels=['True Neg','True Pos'])\n    nr = recall_score(y_test, yp, pos_label=0)\n    np_ = precision_score(y_test, yp, pos_label=0, zero_division=0)\n    au = roc_auc_score(y_test, ypr)\n    ax.set_title(f'{name}\\nNegRec={nr:.3f}  NegPrec={np_:.3f}  AUC={au:.3f}')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "shlviy9prv",
   "source": "from sklearn.manifold import TSNE\n\n# t-SNE visualisation of Word2Vec embedding space (colour-coded by true label)\nN_VIZ = 5000\nidx_viz = np.random.choice(len(X_w2v_test), N_VIZ, replace=False)\nX_viz = X_w2v_test[idx_viz]\ny_viz = y_test[idx_viz]\n\nprint(f't-SNE on {N_VIZ} test samples...')\nt0 = time.time()\nX_2d = TSNE(n_components=2, random_state=42, perplexity=40, n_iter=1000).fit_transform(X_viz)\nprint(f'Done in {time.time()-t0:.0f}s')\n\nfig, ax = plt.subplots(figsize=(9, 7))\nfor label, color, name in [(1, '#2ecc71', 'Positive'), (0, '#e74c3c', 'Negative')]:\n    m = y_viz == label\n    ax.scatter(X_2d[m, 0], X_2d[m, 1], c=color, alpha=0.35, s=8,\n               label=f'{name} ({m.sum():,})')\nax.set_title('t-SNE of Word2Vec Document Embeddings (5K test sample)', fontsize=12)\nax.legend(markerscale=5)\nax.set_xlabel('t-SNE dim 1'); ax.set_ylabel('t-SNE dim 2')\nplt.tight_layout()\nplt.show()\nprint('Separation reflects that Word2Vec captures sentiment-relevant structure.')\nprint('However, significant overlap shows the limitation of order-blind average pooling.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6g939xidevf",
   "source": "### 11.3 DistilBERT \u2014 Contextual Transformer Embeddings <a id='11-3'></a>\n\n> \u26a1 **GPU Required.** Run this section on [Google Colab](https://colab.research.google.com) with GPU enabled.  \n> Upload `Reviews.csv` to your Google Drive, mount it (see setup cell below), then run the notebook from the beginning.\n\n**Fine-tuning strategy**: We adapt `distilbert-base-uncased` (66M parameters) to the Amazon review domain by training a 2-class linear head on top of the `[CLS]` token embedding, using weighted cross-entropy to address class imbalance:\n\n$$\n\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N} w_{y_i}\\left[y_i \\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i)\\right], \\quad w_c = \\frac{N}{2 N_c}\n$$\n\nWe fine-tune on **50,000 training samples** (\u224811% of training set) for **3 epochs** with:\n- Optimizer: AdamW, lr = 2\u00d710\u207b\u2075, weight decay = 0.01\n- Warmup: 10% of total steps with linear decay\n- Mixed precision (fp16) for 2\u00d7 speed on GPU\n- Max sequence length: 128 tokens (covers ~95% of reviews)\n\n**Why 50K instead of the full training set?**  \nFine-tuning 3 epochs on 454K samples takes ~6h on T4. The 50K subset trains in ~35-50 min and already provides strong in-domain adaptation. Section 11.3.3 explains how to scale up if needed.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xv2vw95mctm",
   "source": "import time\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import Dataset as TorchDataset, DataLoader\nfrom transformers import (\n    DistilBertTokenizerFast,\n    DistilBertForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\nfrom torch.optim import AdamW\n\n# \u2500\u2500\u2500 Hyperparameters \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nN_FT_TRAIN = 50_000    # samples for fine-tuning; increase on A100 / with more time\nN_FT_VAL   = 5_000\nMAX_LEN    = 128       # tokens \u2014 covers ~95% of reviews\nN_EPOCHS   = 3\nLR         = 2e-5\nTRAIN_BS   = 32\nEVAL_BS    = 64\n\n# \u2500\u2500\u2500 Class weights (for imbalance) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nn_neg = (y_ft_train == 0).sum()\nn_pos = (y_ft_train == 1).sum()\nn_tot = len(y_ft_train)\nw0 = n_tot / (2 * n_neg)\nw1 = n_tot / (2 * n_pos)\nclass_weights = torch.tensor([w0, w1], dtype=torch.float).to(device)\nprint(f'Class weights: neg={w0:.3f}  pos={w1:.3f}')\n\n# \u2500\u2500\u2500 PyTorch Dataset \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nclass ReviewDataset(TorchDataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        enc = tokenizer(\n            list(texts), truncation=True, padding='max_length',\n            max_length=max_len, return_tensors='pt'\n        )\n        self.input_ids      = enc['input_ids']\n        self.attention_mask = enc['attention_mask']\n        self.labels         = torch.tensor(labels, dtype=torch.long)\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        return {\n            'input_ids':      self.input_ids[idx],\n            'attention_mask': self.attention_mask[idx],\n            'labels':         self.labels[idx]\n        }\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\nprint('Tokenizing fine-tune datasets...')\nt0 = time.time()\ntrain_ds = ReviewDataset(raw_ft_train, y_ft_train, tokenizer, MAX_LEN)\nval_ds   = ReviewDataset(raw_ft_val,   y_ft_val,   tokenizer, MAX_LEN)\ntest_ds  = ReviewDataset(raw_test,     y_test,     tokenizer, MAX_LEN)\nprint(f'Done in {time.time()-t0:.0f}s')\n\ntrain_loader = DataLoader(train_ds, batch_size=TRAIN_BS, shuffle=True,  num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_ds,   batch_size=EVAL_BS,  shuffle=False, num_workers=2, pin_memory=True)\ntest_loader  = DataLoader(test_ds,  batch_size=EVAL_BS,  shuffle=False, num_workers=2, pin_memory=True)\n\n# \u2500\u2500\u2500 Model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nmodel_bert = DistilBertForSequenceClassification.from_pretrained(\n    'distilbert-base-uncased', num_labels=2,\n    id2label={0:'NEGATIVE', 1:'POSITIVE'},\n    label2id={'NEGATIVE':0, 'POSITIVE':1}\n).to(device)\n\n# \u2500\u2500\u2500 Optimizer + Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntotal_steps   = len(train_loader) * N_EPOCHS\nwarmup_steps  = int(0.1 * total_steps)\n\noptimizer = AdamW(model_bert.parameters(), lr=LR, weight_decay=0.01)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss(weight=class_weights)\n\nn_params = sum(p.numel() for p in model_bert.parameters() if p.requires_grad)\nprint(f'Trainable parameters: {n_params/1e6:.1f}M')\nprint(f'Steps per epoch: {len(train_loader):,}  |  Total steps: {total_steps:,}')\nprint(f'Warmup steps: {warmup_steps}')\nprint(f'\\\\nEstimated time on T4 GPU: ~35-50 min for {N_EPOCHS} epochs')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "w90myd098d",
   "source": "%%time\n# \u2500\u2500 DistilBERT: Training loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfrom tqdm.auto import tqdm\n\nscaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda'))\nhistory = []\n\nfor epoch in range(1, N_EPOCHS + 1):\n    # \u2500\u2500 Train \u2500\u2500\n    model_bert.train()\n    total_loss, n_correct, n_total = 0.0, 0, 0\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{N_EPOCHS} [train]', leave=False)\n    for batch in pbar:\n        input_ids      = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels         = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast(enabled=(device == 'cuda')):\n            outputs = model_bert(input_ids=input_ids, attention_mask=attention_mask)\n            loss = loss_fn(outputs.logits, labels)\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model_bert.parameters(), 1.0)\n        scaler.step(optimizer); scaler.update()\n        scheduler.step()\n\n        total_loss += loss.item() * labels.size(0)\n        n_correct  += (outputs.logits.argmax(dim=-1) == labels).sum().item()\n        n_total    += labels.size(0)\n\n    train_loss = total_loss / n_total\n    train_acc  = n_correct / n_total\n\n    # \u2500\u2500 Validate \u2500\u2500\n    model_bert.eval()\n    val_preds, val_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            logits = model_bert(\n                input_ids=batch['input_ids'].to(device),\n                attention_mask=batch['attention_mask'].to(device)\n            ).logits\n            val_preds.extend(logits.argmax(dim=-1).cpu().numpy())\n            val_labels.extend(batch['labels'].numpy())\n\n    val_neg_f1 = f1_score(val_labels, val_preds, pos_label=0, zero_division=0)\n    val_neg_rec = recall_score(val_labels, val_preds, pos_label=0)\n\n    history.append({'epoch': epoch, 'train_loss': train_loss,\n                    'train_acc': train_acc, 'val_neg_f1': val_neg_f1,\n                    'val_neg_rec': val_neg_rec})\n    print(f'Epoch {epoch}: loss={train_loss:.4f}  acc={train_acc:.4f}  '\n          f'val_NegF1={val_neg_f1:.4f}  val_NegRec={val_neg_rec:.4f}')\n\n# Plot training history\ndf_hist = pd.DataFrame(history)\nfig, axes = plt.subplots(1, 2, figsize=(11, 4))\naxes[0].plot(df_hist['epoch'], df_hist['train_loss'], 'o-', color='#3498db')\naxes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Training Loss')\naxes[0].set_title('DistilBERT Training Loss'); axes[0].grid(alpha=0.3)\naxes[1].plot(df_hist['epoch'], df_hist['val_neg_f1'],  'o-', color='#2ecc71', label='Val Neg F1')\naxes[1].plot(df_hist['epoch'], df_hist['val_neg_rec'], 's-', color='#e67e22', label='Val Neg Recall')\naxes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Score')\naxes[1].set_title('Validation Metrics per Epoch')\naxes[1].legend(); axes[1].grid(alpha=0.3)\nplt.tight_layout(); plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "167i51oi3s1",
   "source": "# \u2500\u2500 DistilBERT: Evaluation on full test set \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport torch.nn.functional as F\n\nprint('Evaluating on full test set...')\nmodel_bert.eval()\n\nall_logits = []\nEVAL_BS = 64\n\nfor i in range(0, len(test_dataset), EVAL_BS):\n    batch = test_dataset[i:i+EVAL_BS]\n    input_ids      = torch.tensor(batch['input_ids']).to(device)\n    attention_mask = torch.tensor(batch['attention_mask']).to(device)\n    with torch.no_grad():\n        out = model_bert(input_ids=input_ids, attention_mask=attention_mask)\n    all_logits.append(out.logits.cpu())\n\nlogits_all = torch.cat(all_logits, dim=0)\nprobs_all  = F.softmax(logits_all, dim=-1).numpy()\npreds_bert = np.argmax(probs_all, axis=-1)\ny_prob_bert = probs_all[:, 1]   # p(positive)\n\nprint('\\n=== DistilBERT Fine-tuned \u2014 Full Test Set ===')\nprint(classification_report(y_test, preds_bert, target_names=['Negative','Positive'], digits=4))\nprint(f'AUC-ROC: {roc_auc_score(y_test, y_prob_bert):.4f}')\n\n# --- Threshold tuning for BERT ---\nfrom sklearn.metrics import precision_recall_curve as prc\nprecisions_b, recalls_b, thresholds_b = prc(y_test, 1 - y_prob_bert, pos_label=1)\nf1_b = 2 * precisions_b[:-1] * recalls_b[:-1] / (precisions_b[:-1] + recalls_b[:-1] + 1e-8)\nbest_b_idx = np.argmax(f1_b)\ntau_b = 1 - thresholds_b[best_b_idx]\npreds_bert_tuned = (y_prob_bert < tau_b).astype(int)\nprint(f'\\nWith threshold \u03c4* = {tau_b:.3f}:')\nprint(f'  Neg Recall   : {recall_score(y_test, preds_bert_tuned, pos_label=0):.4f}')\nprint(f'  Neg Precision: {precision_score(y_test, preds_bert_tuned, pos_label=0, zero_division=0):.4f}')\nprint(f'  Neg F1       : {f1_score(y_test, preds_bert_tuned, pos_label=0):.4f}')\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\ncm_base = confusion_matrix(y_test, preds_bert)\ncm_tuned = confusion_matrix(y_test, preds_bert_tuned)\nfor ax, cm, title in [(axes[0], cm_base, f'DistilBERT (\u03c4=0.5)'),\n                      (axes[1], cm_tuned, f'DistilBERT (\u03c4*={tau_b:.2f})')]:\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=ax,\n                xticklabels=['Pred Neg','Pred Pos'],\n                yticklabels=['True Neg','True Pos'])\n    ax.set_title(title)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0m8jmhv5bs6n",
   "source": "# \u2500\u2500 11.4 Embedding Method Comparison \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Aggregates results from W2V and BERT (runs after 11.2 and 11.3)\n\ndef eval_row(name, y_true, y_pred, y_prob):\n    return {\n        'Model': name,\n        'Accuracy':      round((y_true == y_pred).mean(), 4),\n        'Neg Recall':    round(recall_score(y_true, y_pred, pos_label=0), 4),\n        'Neg Precision': round(precision_score(y_true, y_pred, pos_label=0, zero_division=0), 4),\n        'Neg F1':        round(f1_score(y_true, y_pred, pos_label=0, zero_division=0), 4),\n        'AUC-ROC':       round(roc_auc_score(y_true, y_prob), 4),\n    }\n\nembed_rows = [\n    eval_row('TF-IDF + LR (Balanced+EN)',    y_test, y_pred_best_lr,  y_prob_best_lr),\n    eval_row('Word2Vec + LR (Balanced)',      y_test, y_pred_w2v_lr,   y_prob_w2v_lr),\n    eval_row('Word2Vec + RF (Balanced)',      y_test, y_pred_w2v_rf,   y_prob_w2v_rf),\n]\ntry:\n    embed_rows.append(eval_row('DistilBERT (fine-tuned)', y_test, preds_bert, y_prob_bert))\nexcept NameError:\n    print('DistilBERT results not yet available (run Section 11.3 on GPU first).')\n\ndf_embed = pd.DataFrame(embed_rows).sort_values('Neg F1', ascending=False)\nprint('=== Embedding Method Comparison ===')\nprint(df_embed.to_string(index=False))\n\n# ROC curve overlay\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\nax = axes[0]\nroc_data = [\n    ('TF-IDF + LR',    y_prob_best_lr, '#3498db', '-'),\n    ('Word2Vec + LR',  y_prob_w2v_lr,  '#e67e22', '--'),\n    ('Word2Vec + RF',  y_prob_w2v_rf,  '#e74c3c', '-.'),\n]\ntry:\n    roc_data.append(('DistilBERT',  y_prob_bert, '#27ae60', '-'))\nexcept NameError:\n    pass\nfor name, prob, color, ls in roc_data:\n    fpr, tpr, _ = roc_curve(y_test, prob)\n    au = auc(fpr, tpr)\n    ax.plot(fpr, tpr, lw=2, color=color, ls=ls, label=f'{name}  (AUC={au:.3f})')\nax.plot([0,1],[0,1],'k--', lw=0.8)\nax.set_xlabel('FPR'); ax.set_ylabel('TPR')\nax.set_title('ROC Curves \u2014 Embedding Methods')\nax.legend(loc='lower right', fontsize=9); ax.grid(alpha=0.3)\n\n# Bar chart: key metrics\nax2 = axes[1]\nmodels_bar = df_embed['Model'].tolist()\nx = np.arange(len(models_bar)); w = 0.22\nmetrics_bar = [('Neg Recall','#e67e22'), ('Neg Precision','#e74c3c'),\n               ('Neg F1','#2ecc71'), ('AUC-ROC','#3498db')]\nfor i, (m, c) in enumerate(metrics_bar):\n    vals = [df_embed.loc[df_embed['Model']==n, m].values[0] for n in models_bar]\n    ax2.bar(x + (i-1.5)*w, vals, w, label=m, color=c, edgecolor='white')\nax2.set_xticks(x)\nax2.set_xticklabels(models_bar, rotation=15, ha='right', fontsize=9)\nax2.set_ylim(0.55, 1.0); ax2.set_ylabel('Score')\nax2.set_title('Metric Comparison \u2014 All Embedding Methods')\nax2.legend(loc='lower right', fontsize=8); ax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-s12",
   "metadata": {},
   "source": "---\n## 12. Ensemble Learning \u2014 Soft Voting <a id='12'></a>\n\n### Motivation\n\nThe two best base models have **complementary strengths**:\n\n| Model | Neg. Recall | Neg. Precision |\n|---|---|---|\n| Best LR (Balanced + ElasticNet) | High \u2191 | Lower \u2193 |\n| RF (Balanced + SVD) | Lower \u2193 | High \u2191 |\n\n**Soft Voting** combines them by averaging predicted class probabilities:\n\n$$\n\\hat{P}_{\\text{ens}}(c \\mid \\mathbf{x}) = \\frac{1}{2}\\Big[\\hat{P}_{\\text{LR}}(c \\mid \\mathbf{x}) + \\hat{P}_{\\text{RF}}(c \\mid \\mathbf{x})\\Big]\n$$\n\n$$\n\\hat{y} = \\underset{c}{\\arg\\max}\\; \\hat{P}_{\\text{ens}}(c \\mid \\mathbf{x})\n$$\n\nThis is superior to **hard voting** (majority vote of predicted labels) because it uses full probability information from each model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-voting",
   "metadata": {},
   "outputs": [],
   "source": "# Soft Voting on SVD-reduced features (both models need same input)\n# We need to retrain the LR on SVD features for the VotingClassifier\nlr_svd = LogisticRegression(\n    penalty='elasticnet', solver='saga', l1_ratio=grid.best_params_['l1_ratio'],\n    class_weight='balanced', max_iter=1000, random_state=42\n)\nlr_svd.fit(X_train_svd, y_train)\n\nvoting_clf = VotingClassifier(\n    estimators=[('lr', lr_svd), ('rf', best_rf)],\n    voting='soft',\n    n_jobs=-1\n)\nvoting_clf.fit(X_train_svd, y_train)\n\ny_pred_vote = voting_clf.predict(X_test_svd)\ny_prob_vote = voting_clf.predict_proba(X_test_svd)[:, 1]\n\nprint('=== Soft Voting Ensemble (LR + RF) ===')\nprint(classification_report(y_test, y_pred_vote, target_names=['Negative','Positive'], digits=4))\nprint(f'AUC-ROC: {roc_auc_score(y_test, y_prob_vote):.4f}')\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\nplot_evaluation(y_test, y_prob_vote, y_pred_vote,\n                'Soft Voting Ensemble', axes[0], axes[1])\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s13",
   "metadata": {},
   "source": "---\n## 13. Model Comparison & Selection <a id='13'></a>"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare",
   "metadata": {},
   "outputs": [],
   "source": "all_models = {\n    'LR Baseline':                    (y_test, y_pred_base,     y_prob_base),\n    'LR + Class Weights':             (y_test, y_pred_cw,       y_prob_cw),\n    'LR + SMOTE (SVD)':               (y_test, y_pred_smote,    y_prob_smote),\n    'LR + Undersampling':             (y_test, y_pred_rus,      y_prob_rus),\n    'LR + Balanced + ElasticNet':     (y_test, y_pred_best_lr,  y_prob_best_lr),\n    'LR + Balanced + Threshold':      (y_test, y_pred_tuned,    y_prob_best_lr),\n    'RF + Balanced + SVD':            (y_test, y_pred_rf,       y_prob_rf),\n    'Soft Voting (LR + RF)':          (y_test, y_pred_vote,     y_prob_vote),\n    # Word2Vec embeddings (populated after Section 11.2)\n    'Word2Vec + LR (Balanced)':       (y_test, y_pred_w2v_lr,   y_prob_w2v_lr),\n    'Word2Vec + RF (Balanced)':       (y_test, y_pred_w2v_rf,   y_prob_w2v_rf),\n}\n\n# Add DistilBERT if available (Section 11.3, GPU required)\ntry:\n    all_models['DistilBERT (fine-tuned)'] = (y_test, preds_bert, y_prob_bert)\nexcept NameError:\n    print('DistilBERT not yet run (Section 11.3 requires GPU). Skipping.')\n\nrows = []\nfor name, (yt, yp, ypr) in all_models.items():\n    rows.append({\n        'Model': name,\n        'Accuracy': round((yt==yp).mean(), 4),\n        'Neg Recall': round(recall_score(yt, yp, pos_label=0), 4),\n        'Neg Precision': round(precision_score(yt, yp, pos_label=0, zero_division=0), 4),\n        'Neg F1': round(f1_score(yt, yp, pos_label=0, zero_division=0), 4),\n        'AUC': round(roc_auc_score(yt, ypr), 4),\n    })\n\ndf_compare = pd.DataFrame(rows).sort_values('AUC', ascending=False)\nprint(df_compare.to_string(index=False))\n\n# --- ROC curves overlay ---\nfig, ax = plt.subplots(figsize=(9, 6.5))\ncolors = plt.cm.tab10.colors\nfor i, (name, (yt, yp, ypr)) in enumerate(all_models.items()):\n    fpr, tpr, _ = roc_curve(yt, ypr)\n    roc_auc_val = auc(fpr, tpr)\n    ls = '--' if 'Word2Vec' in name or 'BERT' in name else '-'\n    ax.plot(fpr, tpr, lw=1.8, ls=ls, color=colors[i % len(colors)],\n            label=f'{name} ({roc_auc_val:.3f})')\nax.plot([0,1],[0,1],'k--', lw=0.8)\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC Curves \u2014 All Models')\nax.legend(loc='lower right', fontsize=7)\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s14",
   "metadata": {},
   "source": "---\n## 14. Feature Importance & Interpretability <a id='14'></a>\n\nLogistic Regression coefficients directly quantify each feature's contribution to the log-odds of the positive class:\n\n$$\n\\log\\frac{P(y=1)}{P(y=0)} = \\sum_j w_j x_j + b\n$$\n\n- **Large positive $w_j$**: feature $j$ strongly predicts positive sentiment.\n- **Large negative $w_j$**: feature $j$ strongly predicts negative sentiment.\n\nThis provides transparent, word-level explanations \u2014 critical for business stakeholders."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-importance",
   "metadata": {},
   "outputs": [],
   "source": "feature_names = vectorizer.get_feature_names_out()\ncoefs = best_lr.coef_.flatten()\n\ncoef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefs})\ncoef_df = coef_df.sort_values('Coefficient', ascending=False)\n\ntop_n = 20\ntop_pos = coef_df.head(top_n)\ntop_neg = coef_df.tail(top_n).iloc[::-1]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 7))\n\naxes[0].barh(top_pos['Feature'], top_pos['Coefficient'],\n             color='#2ecc71', edgecolor='none')\naxes[0].set_title(f'Top {top_n} Positive Sentiment Features')\naxes[0].set_xlabel('LR Coefficient')\naxes[0].invert_yaxis()\naxes[0].grid(axis='x', alpha=0.3)\n\naxes[1].barh(top_neg['Feature'], top_neg['Coefficient'],\n             color='#e74c3c', edgecolor='none')\naxes[1].set_title(f'Top {top_n} Negative Sentiment Features')\naxes[1].set_xlabel('LR Coefficient')\naxes[1].invert_yaxis()\naxes[1].grid(axis='x', alpha=0.3)\n\nplt.suptitle('Logistic Regression Feature Coefficients', fontsize=13, y=1.01)\nplt.tight_layout()\nplt.show()\n\nprint('Top 10 POSITIVE features:')\nprint(coef_df.head(10)[['Feature','Coefficient']].to_string(index=False))\nprint('\\nTop 10 NEGATIVE features:')\nprint(coef_df.tail(10).iloc[::-1][['Feature','Coefficient']].to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-s15",
   "metadata": {},
   "source": "---\n## 15. Conclusions <a id='15'></a>\n\n### Key Findings\n\n1. **Stop-word handling matters critically** for sentiment analysis. Retaining negation words (\"not\", \"won't\", \"can't\") is essential \u2014 removing them inverts sentiment in phrases like \"not great\" and \"won't buy again\".\n\n2. **Baseline-first development** is essential for measuring the true impact of each technique. Starting with SVD or aggressive resampling without a baseline makes it impossible to know if the changes helped.\n\n3. **SVD is not appropriate as primary dimensionality reduction for TF-IDF NLP**: 300 components explain only ~30% of variance; 1,000 explain < 70%. Use SVD only as a necessary preprocessing step for algorithms requiring dense input (e.g., SMOTE).\n\n4. **Class weighting is the most effective and efficient** imbalance-handling strategy: no data manipulation, no information loss, best precision-recall balance among all strategies.\n\n5. **Threshold tuning is underused**: after fitting a model, adjusting the decision threshold (tau* = 0.37) is a zero-cost way to control the precision-recall trade-off for deployment.\n\n6. **The Soft Voting ensemble** achieves the best AUC (0.951) among TF-IDF-based models by combining complementary strengths of the high-recall LR and high-precision RF.\n\n7. **Word2Vec embeddings** (Skip-gram, d=300) provide denser semantic representations than TF-IDF, capturing word similarity and analogy, but average pooling loses word-order information. On this dataset they typically match but do not dramatically exceed TF-IDF+LR in classification accuracy.\n\n8. **DistilBERT fine-tuning** yields the best overall performance \u2014 contextual embeddings capture negation, sarcasm, and domain vocabulary that bag-of-words methods miss. The trade-off is orders-of-magnitude higher compute (GPU required) vs. TF-IDF models that train in seconds on CPU.\n\n9. **Embedding quality scales with representation complexity**: TF-IDF (sparse, count-based) < Word2Vec (dense, static) < DistilBERT (dense, contextual). Each step up improves semantics at the cost of training time and infrastructure.\n\n### Embedding Comparison Summary\n\n| Representation | AUC-ROC | Neg. F1 | Training Time | GPU Required |\n|---|---|---|---|---|\n| TF-IDF + LR (baseline) | 0.942 | 0.753 | Seconds | No |\n| TF-IDF + LR (balanced, elastic) | 0.951 | 0.768 | Seconds | No |\n| TF-IDF Ensemble (Soft Voting) | 0.951 | \u2014 | Minutes | No |\n| Word2Vec + LR (balanced) | See \u00a711 | See \u00a711 | ~5 min | No |\n| Word2Vec + RF (balanced) | See \u00a711 | See \u00a711 | ~10 min | No |\n| DistilBERT (fine-tuned) | See \u00a711 | See \u00a711 | ~30-60 min | Yes |\n\n### Final Model Recommendation\n\n**For production with resource constraints**: Use the **Soft Voting ensemble** (TF-IDF + LR + RF) with **threshold tuning** (tau* = 0.37). This achieves AUC=0.951, trains in minutes on CPU, and requires no GPU.\n\n**For maximum accuracy with GPU available**: Fine-tune **DistilBERT** on the full dataset. Expected AUC > 0.97 with superior handling of negation, sarcasm, and complex product descriptions.\n\n**For a middle ground**: **Word2Vec + Logistic Regression** (balanced class weights) provides better semantic representations than TF-IDF without requiring GPU \u2014 a good option when embedding quality matters but transformer compute is unavailable.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-save",
   "metadata": {},
   "outputs": [],
   "source": "# Save the best TF-IDF-based models\nimport joblib, os\njoblib.dump(best_lr,       '../models/best_logistic_model.pkl')\njoblib.dump(best_rf,       '../models/best_rf.pkl')\njoblib.dump(voting_clf,    '../models/voting_classifier_model.pkl')\njoblib.dump(svd,           '../models/svd_1000.pkl')\njoblib.dump(vectorizer,    '../models/tfidf_vectorizer.pkl')\nprint('TF-IDF models saved.')\n\n# Save Word2Vec model (if trained in Section 11)\ntry:\n    w2v_model.wv.save_word2vec_format('../models/w2v_300d.bin', binary=True)\n    joblib.dump(lr_w2v, '../models/w2v_lr_model.pkl')\n    joblib.dump(rf_w2v, '../models/w2v_rf_model.pkl')\n    print('Word2Vec models saved.')\nexcept NameError:\n    print('Word2Vec models not found (Section 11 not run) -- skipping.')\n\n# Save fine-tuned DistilBERT (if trained in Section 11)\ntry:\n    bert_save_path = '../models/distilbert_finetuned'\n    os.makedirs(bert_save_path, exist_ok=True)\n    model_bert.save_pretrained(bert_save_path)\n    tokenizer.save_pretrained(bert_save_path)\n    print(f'DistilBERT model saved to {bert_save_path}/')\nexcept NameError:\n    print('DistilBERT model not found (Section 11 not run) -- skipping.')\n\nprint('Done. All available models saved to ../models/')\n"
  }
 ]
}