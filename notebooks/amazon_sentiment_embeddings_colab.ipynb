{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Amazon Fine Food Reviews — Embedding Comparison\n",
    "\n",
    "**Author:** Ricky Gong, University of Pennsylvania (`gong8@seas.upenn.edu`)  \n",
    "**Runtime:** Google Colab — **GPU (T4 or better) required for DistilBERT sections**\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook compares four text representation strategies for binary sentiment classification on the Amazon Fine Food Reviews dataset:\n",
    "\n",
    "| # | Method | Type | GPU Required |\n",
    "|---|---|---|---|\n",
    "| 1 | TF-IDF + Logistic Regression | Sparse, count-based | No |\n",
    "| 2 | Word2Vec (avg. pooling) + LR/RF | Dense, static embeddings | No |\n",
    "| 3 | DistilBERT (zero-shot, pre-trained SST-2) | Contextual transformer | Yes (inference) |\n",
    "| 4 | DistilBERT fine-tuned on Amazon reviews | Contextual transformer | Yes (training) |\n",
    "\n",
    "### Research Question\n",
    "\n",
    "> Do dense word embeddings and contextual representations provide a significant improvement over classical TF-IDF features for binary sentiment classification under class imbalance?\n",
    "\n",
    "### Key Hypotheses\n",
    "\n",
    "1. **Word2Vec** captures semantic similarity (e.g., "terrible" ≈ "awful") that TF-IDF misses → better generalisation.\n",
    "2. **DistilBERT** (contextual) handles negation and sarcasm in context → higher recall for the negative class.\n",
    "3. The improvement from TF-IDF → Word2Vec → DistilBERT follows the expressiveness of the representations.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Installation](#1)\n",
    "2. [Data Loading](#2)\n",
    "3. [Text Preprocessing](#3)\n",
    "4. [TF-IDF Baseline (Quick Repro)](#4)\n",
    "5. [Word2Vec Embeddings](#5)\n",
    "6. [DistilBERT — Zero-shot Evaluation](#6)\n",
    "7. [DistilBERT — Fine-tuning on Amazon Reviews](#7)\n",
    "8. [Full Dataset Fine-tuning (Optional — Long)](#8)\n",
    "9. [Model Comparison & Conclusions](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Installation <a id='1'></a>\n",
    "\n",
    "Run the cell below once at the start of your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (Colab already has torch, sklearn, pandas)\n",
    "!pip install gensim transformers datasets accelerate --quiet\n",
    "!pip install imbalanced-learn --quiet  # for SMOTE if needed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')\n",
    "if device == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('WARNING: No GPU detected. DistilBERT fine-tuning will be very slow.')\n",
    "    print('Go to Runtime > Change Runtime Type > GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, random, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Classical ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, roc_curve,\n",
    "    precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Word2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "plt.rcParams.update({'figure.dpi': 120, 'font.size': 11})\n",
    "sns.set_style('whitegrid')\n",
    "print('All imports successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading <a id='2'></a>\n",
    "\n",
    "**Option A (Recommended):** Upload `Reviews.csv` to your Google Drive and mount it below.\n",
    "\n",
    "**Option B:** Download directly from Kaggle via the API.\n",
    "\n",
    "Choose one option and run the corresponding cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-drive-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── OPTION A: Google Drive ──────────────────────────────────────────────────\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Adjust this path to where you placed Reviews.csv in your Drive:\n",
    "DATA_PATH = '/content/drive/MyDrive/amazon_reviews/Reviews.csv'\n",
    "\n",
    "# ── OPTION B: Kaggle API ─────────────────────────────────────────────────────\n",
    "# Uncomment and fill in your Kaggle credentials:\n",
    "# import os\n",
    "# os.environ['KAGGLE_USERNAME'] = 'YOUR_KAGGLE_USERNAME'\n",
    "# os.environ['KAGGLE_KEY'] = 'YOUR_KAGGLE_API_KEY'\n",
    "# !pip install kaggle --quiet\n",
    "# !kaggle datasets download -d snap/amazon-fine-food-reviews -p /content/\n",
    "# !unzip -q /content/amazon-fine-food-reviews.zip -d /content/\n",
    "# DATA_PATH = '/content/Reviews.csv'\n",
    "\n",
    "print(f'Data path: {DATA_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print(f'Raw dataset shape: {df_raw.shape}')\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s3-md",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Text Preprocessing <a id='3'></a>\n",
    "\n",
    "Same preprocessing pipeline as the main notebook:\n",
    "1. Binary label construction (exclude score = 3)\n",
    "2. Custom stop-word list **preserving negation words**\n",
    "3. Lowercasing + punctuation removal + Snowball stemming\n",
    "\n",
    "**Critical**: We keep negation words ("not", "won't", "can't") because they carry strong sentiment signal. Removing them would invert phrases like "not great" → "great"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-label-build",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary labels: exclude score == 3\n",
    "df = df_raw[df_raw['Score'] != 3].copy()\n",
    "df['Sentiment'] = (df['Score'] > 3).astype(int)  # 1=positive, 0=negative\n",
    "\n",
    "counts = df['Sentiment'].value_counts()\n",
    "print(f'Dataset after label construction:')\n",
    "print(f'  Positive (1): {counts[1]:>7,}  ({counts[1]/len(df)*100:.1f}%)')\n",
    "print(f'  Negative (0): {counts[0]:>7,}  ({counts[0]/len(df)*100:.1f}%)')\n",
    "print(f'  Total       : {len(df):>7,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stop-word list that PRESERVES negation\n",
    "all_stop = set(stopwords.words('english'))\n",
    "negation_words = {\n",
    "    'not', 'no', 'nor', 'never', 'neither', 'nobody', 'nothing', 'nowhere',\n",
    "    \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\",\n",
    "    \"can't\", \"cannot\", \"couldn't\", \"shouldn't\",\n",
    "    \"isn't\", \"aren't\", \"wasn't\", \"weren't\",\n",
    "    \"haven't\", \"hasn't\", \"hadn't\", \"ain't\",\n",
    "    \"mightn't\", \"mustn't\", \"needn't\"\n",
    "}\n",
    "custom_stop = all_stop - negation_words\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[?!.,;:)(|/\"#]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    return ' '.join(\n",
    "        stemmer.stem(tok) for tok in tokens if tok not in custom_stop\n",
    "    )\n",
    "\n",
    "print('Applying preprocessing... (this takes ~2-3 min)')\n",
    "t0 = time.time()\n",
    "df['CleanText'] = df['Text'].fillna('').apply(clean_text)\n",
    "print(f'Done in {time.time()-t0:.0f}s')\n",
    "\n",
    "# Verify\n",
    "print('\\nSample:')\n",
    "print('Original:', df['Text'].iloc[0][:100])\n",
    "print('Cleaned :', df['CleanText'].iloc[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split — stratified, 80/20\n",
    "texts = df['CleanText'].values\n",
    "labels = df['Sentiment'].values\n",
    "raw_texts = df['Text'].fillna('').values  # raw text for BERT\n",
    "\n",
    "(\n",
    "    texts_train, texts_test,\n",
    "    raw_train, raw_test,\n",
    "    y_train, y_test\n",
    ") = train_test_split(\n",
    "    texts, raw_texts, labels,\n",
    "    test_size=0.20, random_state=SEED, stratify=labels\n",
    ")\n",
    "\n",
    "print(f'Train : {len(y_train):,}  (pos={y_train.sum():,}, neg={(y_train==0).sum():,})')\n",
    "print(f'Test  : {len(y_test):,}  (pos={y_test.sum():,}, neg={(y_test==0).sum():,})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s4-md",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. TF-IDF Baseline (Quick Repro) <a id='4'></a>\n",
    "\n",
    "We re-establish the TF-IDF + LR baseline from the main notebook to serve as the comparison anchor for all subsequent methods.\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t,d,D) = \\log(1 + \\text{TF}(t,d)) \\times \\log\\!\\left(\\frac{|D|+1}{|\\{d:t\\in d\\}|+1}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-tfidf-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TF-IDF vectorizer (same settings as main notebook)\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=10_000,\n",
    "    min_df=5,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_tfidf_train = tfidf.fit_transform(texts_train)\n",
    "X_tfidf_test  = tfidf.transform(texts_test)\n",
    "print(f'TF-IDF matrix: {X_tfidf_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-tfidf-lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Logistic Regression with class weights (best classical setup from main notebook)\n",
    "lr_tfidf = LogisticRegression(\n",
    "    penalty='elasticnet', solver='saga', l1_ratio=0.5,\n",
    "    class_weight='balanced', max_iter=1000, random_state=SEED\n",
    ")\n",
    "lr_tfidf.fit(X_tfidf_train, y_train)\n",
    "\n",
    "y_pred_tfidf = lr_tfidf.predict(X_tfidf_test)\n",
    "y_prob_tfidf = lr_tfidf.predict_proba(X_tfidf_test)[:, 1]\n",
    "\n",
    "print('=== TF-IDF + LR (Balanced + ElasticNet) ===')\n",
    "print(classification_report(y_test, y_pred_tfidf,\n",
    "                            target_names=['Negative','Positive'], digits=4))\n",
    "print(f'AUC-ROC: {roc_auc_score(y_test, y_prob_tfidf):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s5-md",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Word2Vec Embeddings <a id='5'></a>\n",
    "\n",
    "### 5.1 Theory\n",
    "\n",
    "Word2Vec (Mikolov et al., 2013) learns a dense embedding $\\mathbf{e}_w \\in \\mathbb{R}^d$ for each word $w$ such that semantically similar words are mapped to nearby vectors:\n",
    "\n",
    "$$\n",
    "\\cos(\\mathbf{e}_{\\text{terrible}},\\, \\mathbf{e}_{\\text{awful}}) \\approx 1, \\quad \\cos(\\mathbf{e}_{\\text{terrible}},\\, \\mathbf{e}_{\\text{delicious}}) \\approx 0\n",
    "$$\n",
    "\n",
    "**Skip-gram objective** (predicts context words from a target word):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t} \\sum_{c \\in \\mathcal{C}(t)} \\log P(w_c \\mid w_t)\n",
    "= -\\sum_t \\sum_{c \\in \\mathcal{C}(t)} \\log \\sigma(\\mathbf{e}_{w_t}^\\top \\mathbf{e}_{w_c})\n",
    "$$\n",
    "\n",
    "### 5.2 Document Representation via Average Pooling\n",
    "\n",
    "A document $d = (w_1, \\ldots, w_n)$ is represented as the average of its word embeddings:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_d = \\frac{1}{|V_d|} \\sum_{w \\in V_d} \\mathbf{e}_w\n",
    "$$\n",
    "\n",
    "where $V_d$ is the vocabulary of words in $d$ that appear in the trained model. This produces a dense, fixed-dimensional vector regardless of document length.\n",
    "\n",
    "### 5.3 Limitation\n",
    "\n",
    "Average pooling loses **word order** and **context**. \"not great\" and \"not terrible\" have opposite sentiments but similar average embeddings if the word vectors are close. This is the key weakness that contextual models (BERT) address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-w2v-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Tokenize for Word2Vec (list of token lists)\n",
    "print('Tokenizing training corpus...')\n",
    "train_sentences = [text.split() for text in texts_train]\n",
    "test_sentences  = [text.split() for text in texts_test]\n",
    "\n",
    "# Train Word2Vec on the training corpus only (no test data leakage)\n",
    "print('Training Word2Vec (Skip-gram, d=300, window=5, ~5-10 min)...')\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=train_sentences,\n",
    "    vector_size=300,      # embedding dimension\n",
    "    window=5,             # context window size\n",
    "    min_count=5,          # ignore rare words\n",
    "    sg=1,                 # Skip-gram (sg=1) vs CBOW (sg=0)\n",
    "    workers=4,\n",
    "    epochs=5,\n",
    "    seed=SEED\n",
    ")\n",
    "print(f'Vocabulary size: {len(w2v_model.wv):,} words')\n",
    "print(f'Embedding shape: {w2v_model.wv.vectors.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-w2v-inspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect learned embeddings\n",
    "print('Words most similar to \"delicious\":',\n",
    "      [w for w, _ in w2v_model.wv.most_similar('delici', topn=5)])\n",
    "print('Words most similar to \"disappoint\":',\n",
    "      [w for w, _ in w2v_model.wv.most_similar('disappoint', topn=5)])\n",
    "print('Words most similar to \"not\":',\n",
    "      [w for w, _ in w2v_model.wv.most_similar('not', topn=5)])\n",
    "\n",
    "# Semantic arithmetic: king - man + woman ≈ queen analogy style\n",
    "try:\n",
    "    result = w2v_model.wv.most_similar(\n",
    "        positive=['great', 'not'], negative=['good'], topn=3\n",
    "    )\n",
    "    print(f'\\n\"great\" + \"not\" - \"good\" ≈ {result}')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-w2v-docvec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_vec(tokens: list, model: Word2Vec, dim: int = 300) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Average pooling: compute mean embedding over all known tokens in the document.\n",
    "    Returns a zero vector if no tokens are in the vocabulary.\n",
    "    \"\"\"\n",
    "    vecs = [\n",
    "        model.wv[tok] for tok in tokens if tok in model.wv\n",
    "    ]\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "    return np.mean(vecs, axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "print('Building document vectors...')\n",
    "t0 = time.time()\n",
    "X_w2v_train = np.vstack([doc_to_vec(s, w2v_model) for s in train_sentences])\n",
    "X_w2v_test  = np.vstack([doc_to_vec(s, w2v_model) for s in test_sentences])\n",
    "print(f'Done in {time.time()-t0:.0f}s')\n",
    "print(f'X_w2v_train: {X_w2v_train.shape}  (dense, float32)')\n",
    "print(f'Zero-vector documents (OOV): '\n",
    "      f'{(X_w2v_train.sum(axis=1) == 0).sum()} / {len(X_w2v_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-w2v-lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Logistic Regression on Word2Vec embeddings\n",
    "lr_w2v = LogisticRegression(\n",
    "    max_iter=1000, random_state=SEED,\n",
    "    class_weight='balanced', solver='lbfgs'\n",
    ")\n",
    "lr_w2v.fit(X_w2v_train, y_train)\n",
    "\n",
    "y_pred_w2v_lr = lr_w2v.predict(X_w2v_test)\n",
    "y_prob_w2v_lr = lr_w2v.predict_proba(X_w2v_test)[:, 1]\n",
    "\n",
    "print('=== Word2Vec (avg. pool) + LR (Balanced) ===')\n",
    "print(classification_report(y_test, y_pred_w2v_lr,\n",
    "                            target_names=['Negative','Positive'], digits=4))\n",
    "print(f'AUC-ROC: {roc_auc_score(y_test, y_prob_w2v_lr):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-w2v-rf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Random Forest on Word2Vec embeddings\n",
    "rf_w2v = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=20,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_w2v.fit(X_w2v_train, y_train)\n",
    "\n",
    "y_pred_w2v_rf = rf_w2v.predict(X_w2v_test)\n",
    "y_prob_w2v_rf = rf_w2v.predict_proba(X_w2v_test)[:, 1]\n",
    "\n",
    "print('=== Word2Vec (avg. pool) + Random Forest ===')\n",
    "print(classification_report(y_test, y_pred_w2v_rf,\n",
    "                            target_names=['Negative','Positive'], digits=4))\n",
    "print(f'AUC-ROC: {roc_auc_score(y_test, y_prob_w2v_rf):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-w2v-tsne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualisation of Word2Vec embedding space (sentiment-coloured)\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Use a small subsample for visualisation\n",
    "N_VIZ = 5000\n",
    "idx = np.random.choice(len(X_w2v_test), N_VIZ, replace=False)\n",
    "X_viz = X_w2v_test[idx]\n",
    "y_viz = y_test[idx]\n",
    "\n",
    "print(f't-SNE on {N_VIZ} test samples (this takes ~2-3 min)...')\n",
    "t0 = time.time()\n",
    "tsne = TSNE(n_components=2, random_state=SEED, perplexity=40, n_iter=1000)\n",
    "X_2d = tsne.fit_transform(X_viz)\n",
    "print(f'Done in {time.time()-t0:.0f}s')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "for label, color, name in [(1, '#27ae60', 'Positive'), (0, '#c0392b', 'Negative')]:\n",
    "    mask = y_viz == label\n",
    "    ax.scatter(X_2d[mask, 0], X_2d[mask, 1],\n",
    "               c=color, alpha=0.4, s=8, label=f'{name} ({mask.sum():,})')\n",
    "ax.set_title('t-SNE of Word2Vec Document Embeddings (test set, 5K sample)',\n",
    "             fontsize=12)\n",
    "ax.legend(markerscale=4)\n",
    "ax.set_xlabel('t-SNE dim 1'); ax.set_ylabel('t-SNE dim 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Good class separation visible → Word2Vec captures sentiment-relevant structure.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s6-md",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. DistilBERT — Zero-shot Evaluation <a id='6'></a>\n",
    "\n",
    "### 6.1 What is DistilBERT?\n",
    "\n",
    "DistilBERT (Sanh et al., 2019) is a distilled version of BERT that retains 97% of BERT's performance at 60% of its size. It uses the **Transformer** self-attention mechanism:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Unlike Word2Vec, DistilBERT produces **contextual** embeddings — the representation of "not" changes depending on surrounding words:\n",
    "\n",
    "| Phrase | Word2Vec | DistilBERT |\n",
    "|---|---|---|\n",
    "| \"not great\" | avg(not_vec, great_vec) | context-aware representation of negation |\n",
    "| \"not terrible\" | avg(not_vec, terrible_vec) | context-aware representation of double-neg |\n",
    "\n",
    "### 6.2 Zero-shot: Pre-trained SST-2 Model\n",
    "\n",
    "We first evaluate `distilbert-base-uncased-finetuned-sst-2-english`, a model already fine-tuned for binary sentiment classification on the Stanford Sentiment Treebank (SST-2). This gives us a zero-shot baseline with **no additional training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dbert-zeroshot-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained sentiment pipeline (no training needed)\n",
    "print('Loading distilbert-base-uncased-finetuned-sst-2-english...')\n",
    "sentiment_pipeline = pipeline(\n",
    "    'text-classification',\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    device=0 if device == 'cuda' else -1,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "print('Model loaded.')\n",
    "\n",
    "# Quick sanity check\n",
    "examples = [\n",
    "    \"This product is absolutely amazing, I love it!\",\n",
    "    \"Terrible quality. Won't buy again. Complete disappointment.\",\n",
    "    \"Not as good as I expected. Pretty mediocre.\"\n",
    "]\n",
    "for text in examples:\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    print(f\"  [{result['label']:8s} {result['score']:.3f}] {text[:70]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dbert-zeroshot-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate zero-shot on a test subset (running full 113K takes ~20 min on T4)\n",
    "# Use 5,000 samples for speed; increase N_ZS for more accurate estimate\n",
    "N_ZS = 5000  # increase to 20000 for better estimate, full test for final paper\n",
    "\n",
    "idx_zs = np.random.choice(len(y_test), N_ZS, replace=False)\n",
    "raw_test_sub = raw_test[idx_zs]\n",
    "y_test_sub   = y_test[idx_zs]\n",
    "\n",
    "print(f'Evaluating zero-shot on {N_ZS:,} test samples...')\n",
    "t0 = time.time()\n",
    "\n",
    "# Batch inference for speed\n",
    "BATCH_SIZE = 64\n",
    "preds_zs, probs_zs = [], []\n",
    "for i in range(0, len(raw_test_sub), BATCH_SIZE):\n",
    "    batch = list(raw_test_sub[i:i+BATCH_SIZE])\n",
    "    # Truncate at 500 chars to speed up tokenisation\n",
    "    batch = [t[:500] for t in batch]\n",
    "    results = sentiment_pipeline(batch)\n",
    "    for r in results:\n",
    "        # SST-2 labels: POSITIVE=1, NEGATIVE=0\n",
    "        pred = 1 if r['label'] == 'POSITIVE' else 0\n",
    "        prob = r['score'] if r['label'] == 'POSITIVE' else 1 - r['score']\n",
    "        preds_zs.append(pred)\n",
    "        probs_zs.append(prob)\n",
    "\n",
    "preds_zs = np.array(preds_zs)\n",
    "probs_zs = np.array(probs_zs)\n",
    "\n",
    "print(f'Evaluated in {time.time()-t0:.0f}s')\n",
    "print(f'\\n=== DistilBERT (zero-shot SST-2) — {N_ZS:,} test samples ===')\n",
    "print(classification_report(y_test_sub, preds_zs,\n",
    "                            target_names=['Negative','Positive'], digits=4))\n",
    "print(f'AUC-ROC: {roc_auc_score(y_test_sub, probs_zs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s7-md",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. DistilBERT — Fine-tuning on Amazon Reviews <a id='7'></a>\n",
    "\n",
    "### 7.1 Why Fine-tune?\n",
    "\n",
    "The SST-2 model was trained on movie reviews. Amazon food reviews have a different vocabulary, writing style, and domain-specific expressions. Fine-tuning adapts the model's representations to the target domain.\n",
    "\n",
    "### 7.2 Training Setup\n",
    "\n",
    "We fine-tune on **50,000 training samples** (≈11% of the full training set) for **3 epochs**. This takes approximately **30-45 minutes on a T4 GPU**. The full dataset fine-tuning is in Section 8.\n",
    "\n",
    "**Loss function** — Weighted cross-entropy to handle class imbalance:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i} w_{y_i} \\left[y_i \\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i)\\right]\n",
    "$$\n",
    "\n",
    "where $w_1 = N/(2N_+)$ and $w_0 = N/(2N_-)$ to balance positive and negative classes.\n",
    "\n",
    "### 7.3 Training Strategy\n",
    "\n",
    "- **Backbone**: `distilbert-base-uncased` (66M parameters)\n",
    "- **Head**: Linear classifier on `[CLS]` token embedding\n",
    "- **Optimizer**: AdamW, lr = 2e-5, weight decay = 0.01\n",
    "- **Scheduler**: Linear warmup (10% steps) + linear decay\n",
    "- **Batch size**: 32 (gradient accumulation × 2 if memory-constrained)\n",
    "- **Max sequence length**: 128 tokens (sufficient for most reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dbert-ft-prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Subsample for fine-tuning ──────────────────────────────────────────────\n",
    "N_FT_TRAIN = 50_000   # training samples (increase to 200K for better results)\n",
    "N_FT_VAL   = 5_000    # validation samples\n",
    "\n",
    "# Stratified subsample\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1,\n",
    "                              train_size=N_FT_TRAIN + N_FT_VAL,\n",
    "                              random_state=SEED)\n",
    "idx_ft, _ = next(sss.split(raw_train, y_train))\n",
    "\n",
    "raw_ft  = raw_train[idx_ft]\n",
    "y_ft    = y_train[idx_ft]\n",
    "\n",
    "# Split into train/val\n",
    "raw_ft_train, raw_ft_val, y_ft_train, y_ft_val = train_test_split(\n",
    "    raw_ft, y_ft, test_size=N_FT_VAL,\n",
    "    random_state=SEED, stratify=y_ft\n",
    ")\n",
    "print(f'Fine-tune train : {len(y_ft_train):,}  '\n",
    "      f'(pos={y_ft_train.sum():,}, neg={(y_ft_train==0).sum():,})')\n",
    "print(f'Validation      : {len(y_ft_val):,}  '\n",
    "      f'(pos={y_ft_val.sum():,}, neg={(y_ft_val==0).sum():,})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dbert-tokenize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "MAX_LEN = 128  # truncate to 128 tokens (most reviews fit; handles ~95%)\n",
    "\n",
    "def tokenize_batch(texts, labels):\n",
    "    enc = tokenizer(\n",
    "        list(texts),\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    enc['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "    return enc\n",
    "\n",
    "# Build HuggingFace Datasets\n",
    "def make_hf_dataset(texts, labels):\n",
    "    d = {'text': list(texts), 'label': list(labels)}\n",
    "    ds = Dataset.from_dict(d)\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(\n",
    "            batch['text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "    ds = ds.map(tokenize_fn, batched=True, batch_size=512)\n",
    "    ds = ds.rename_column('label', 'labels')\n",
    "    ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    return ds\n",
    "\n",
    "print('Tokenizing fine-tune train set...')\n",
    "t0 = time.time()\n",
    "train_dataset = make_hf_dataset(raw_ft_train, y_ft_train)\n",
    "val_dataset   = make_hf_dataset(raw_ft_val,   y_ft_val)\n",
    "test_dataset  = make_hf_dataset(raw_test,      y_test)\n",
    "print(f'Done in {time.time()-t0:.0f}s')\n",
    "print(f'Train dataset: {len(train_dataset):,} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dbert-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DistilBERT for sequence classification (2 labels)\n",
    "model_bert = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2,\n",
    "    id2label={0: 'NEGATIVE', 1: 'POSITIVE'},\n",
    "    label2id={'NEGATIVE': 0, 'POSITIVE': 1}\n",
    ")\n",
    "model_bert.to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model_bert.parameters() if p.requires_grad)\n",
    "print(f'Trainable parameters: {n_params/1e6:.1f}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dbert-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# Custom metrics for HuggingFace Trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'neg_recall': recall_score(labels, preds, pos_label=0),\n",
    "        'neg_precision': precision_score(labels, preds, pos_label=0, zero_division=0),\n",
    "        'neg_f1': f1_score(labels, preds, pos_label=0, zero_division=0),\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/distilbert_amazon_50k',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='linear',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='neg_f1',      # optimise for negative class F1\n",
    "    greater_is_better=True,\n",
    "    logging_steps=100,\n",
    "    fp16=(device == 'cuda'),             # mixed precision for speed\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED,\n",
    "    report_to='none',                    # disable wandb\n",
    ")\n",
    "\n",
    "# Weighted trainer for class imbalance\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer that applies class weights to the cross-entropy loss,\n",
    "    handling the 78%/22% class imbalance.\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Class weights: inversely proportional to class frequency\n",
    "        n_neg = (y_ft_train == 0).sum()\n",
    "        n_pos = (y_ft_train == 1).sum()\n",
    "        n_total = len(y_ft_train)\n",
    "        w0 = n_total / (2 * n_neg)   # weight for negative class\n",
    "        w1 = n_total / (2 * n_pos)   # weight for positive class\n",
    "        weights = torch.tensor([w0, w1], dtype=torch.float).to(logits.device)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print('Trainer configured. Ready to train.')\n",
    "print(f'Training on {len(train_dataset):,} samples × 3 epochs')\n",
    "print(f'Batch size: 32  |  Steps per epoch: {len(train_dataset)//32:,}')\n",
    "print('Estimated time on T4 GPU: ~30-45 min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dbert-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ─── START TRAINING ─────────────────────────────────────────────────────────\n",
    "print('Starting DistilBERT fine-tuning...')\n",
    "trainer.train()\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dbert-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on full test set\n",
    "print('Evaluating on full test set...')\n",
    "t0 = time.time()\n",
    "\n",
    "# Get predictions\n",
    "preds_out = trainer.predict(test_dataset)\n",
    "logits_bert = preds_out.predictions           # shape (N_test, 2)\n",
    "probs_bert  = torch.softmax(\n",
    "    torch.tensor(logits_bert), dim=-1\n",
    ").numpy()\n",
    "preds_bert  = np.argmax(logits_bert, axis=-1)\n",
    "y_prob_bert = probs_bert[:, 1]                 # p(positive)\n",
    "\n",
    "print(f'Evaluation done in {time.time()-t0:.0f}s')\n",
    "print('\\n=== DistilBERT Fine-tuned (50K train, 3 epochs) ===')\n",
    "print(classification_report(y_test, preds_bert,\n",
    "                            target_names=['Negative','Positive'], digits=4))\n",
    "print(f'AUC-ROC: {roc_auc_score(y_test, y_prob_bert):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dbert-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned model to Google Drive\n",
    "SAVE_PATH = '/content/drive/MyDrive/amazon_reviews/distilbert_amazon_50k'\n",
    "trainer.save_model(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "print(f'Model saved to {SAVE_PATH}')\n",
    "\n",
    "# Also save predictions for paper table\n",
    "bert_results = pd.DataFrame({\n",
    "    'y_test': y_test,\n",
    "    'y_pred_bert': preds_bert,\n",
    "    'y_prob_bert': y_prob_bert\n",
    "})\n",
    "bert_results.to_csv('/content/drive/MyDrive/amazon_reviews/bert_predictions.csv', index=False)\n",
    "print('Predictions saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s8-md",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Full Dataset Fine-tuning (Optional — Long) <a id='8'></a>\n",
    "\n",
    "For maximum performance, fine-tune on the full training set (~454K samples). This takes approximately **4–6 hours on T4** or **1–2 hours on A100**.\n",
    "\n",
    "**To run full fine-tuning**, execute the cell below. You can also use Colab Pro+ for longer sessions or use gradient checkpointing to reduce memory.\n",
    "\n",
    "> **Tip**: Use `Colab Pro+` for persistent GPU access. Save checkpoints to Drive every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-full-ft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── OPTIONAL: Full dataset fine-tuning ─────────────────────────────────────\n",
    "# Uncomment to run (takes 4-6h on T4)\n",
    "\n",
    "# FULL_TRAIN_DATASET = make_hf_dataset(raw_train, y_train)\n",
    "#\n",
    "# full_training_args = TrainingArguments(\n",
    "#     output_dir='/content/drive/MyDrive/amazon_reviews/distilbert_amazon_full',\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     learning_rate=2e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     warmup_ratio=0.1,\n",
    "#     evaluation_strategy='epoch',\n",
    "#     save_strategy='epoch',\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model='neg_f1',\n",
    "#     logging_steps=500,\n",
    "#     fp16=True,\n",
    "#     gradient_checkpointing=True,   # reduces memory at cost of ~20% speed\n",
    "#     dataloader_num_workers=4,\n",
    "#     seed=SEED,\n",
    "#     report_to='none',\n",
    "# )\n",
    "#\n",
    "# # Reload clean model\n",
    "# model_full = DistilBertForSequenceClassification.from_pretrained(\n",
    "#     'distilbert-base-uncased', num_labels=2\n",
    "# ).to(device)\n",
    "#\n",
    "# full_trainer = WeightedTrainer(\n",
    "#     model=model_full,\n",
    "#     args=full_training_args,\n",
    "#     train_dataset=FULL_TRAIN_DATASET,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "#\n",
    "# full_trainer.train()\n",
    "\n",
    "print('Full fine-tuning cell is ready. Uncomment to run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-s9-md",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Comparison & Conclusions <a id='9'></a>\n",
    "\n",
    "### 9.1 Summary Table\n",
    "\n",
    "Fill in the TF-IDF results from the main notebook if running independently. The cell below assembles all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(y_true, y_pred, y_prob):\n",
    "    return {\n",
    "        'Accuracy':      round((y_true == y_pred).mean(), 4),\n",
    "        'Neg Recall':    round(recall_score(y_true, y_pred, pos_label=0), 4),\n",
    "        'Neg Precision': round(precision_score(y_true, y_pred, pos_label=0, zero_division=0), 4),\n",
    "        'Neg F1':        round(f1_score(y_true, y_pred, pos_label=0, zero_division=0), 4),\n",
    "        'AUC-ROC':       round(roc_auc_score(y_true, y_prob), 4),\n",
    "    }\n",
    "\n",
    "results = {\n",
    "    'TF-IDF + LR (Balanced+EN)':      eval_metrics(y_test, y_pred_tfidf,   y_prob_tfidf),\n",
    "    'Word2Vec (avg) + LR (Balanced)':  eval_metrics(y_test, y_pred_w2v_lr,  y_prob_w2v_lr),\n",
    "    'Word2Vec (avg) + RF (Balanced)':  eval_metrics(y_test, y_pred_w2v_rf,  y_prob_w2v_rf),\n",
    "    'DistilBERT (zero-shot SST-2)':    eval_metrics(y_test_sub, preds_zs,   probs_zs),  # subset\n",
    "    'DistilBERT (fine-tuned 50K)':     eval_metrics(y_test, preds_bert,     y_prob_bert),\n",
    "    # Uncomment after full fine-tuning:\n",
    "    # 'DistilBERT (fine-tuned full)':  eval_metrics(y_test, preds_full, y_prob_full),\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results = df_results.sort_values('Neg F1', ascending=False)\n",
    "print('=== Embedding Method Comparison (Negative Class Metrics) ===')\n",
    "print(df_results.to_string())\n",
    "print('\\nNote: zero-shot metrics are on a 5K subset, not the full test set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compare-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation — metric comparison bar chart\n",
    "methods = list(df_results.index)\n",
    "metrics = ['Neg Recall', 'Neg Precision', 'Neg F1', 'AUC-ROC']\n",
    "colors  = ['#e67e22', '#e74c3c', '#2ecc71', '#3498db']\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "w = 0.18\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    vals = [df_results.loc[m, metric] for m in methods]\n",
    "    ax.bar(x + (i - 1.5) * w, vals, w, label=metric, color=color, edgecolor='white')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods, rotation=20, ha='right', fontsize=9)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Embedding Method Comparison — Negative Class Metrics', fontsize=12)\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-roc-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "roc_models = [\n",
    "    ('TF-IDF + LR',          y_test,     y_prob_tfidf,  '#3498db'),\n",
    "    ('Word2Vec + LR',         y_test,     y_prob_w2v_lr, '#e67e22'),\n",
    "    ('Word2Vec + RF',         y_test,     y_prob_w2v_rf, '#e74c3c'),\n",
    "    ('DistilBERT (zero-shot)', y_test_sub, probs_zs,     '#9b59b6'),\n",
    "    ('DistilBERT (fine-tuned)', y_test,   y_prob_bert,   '#27ae60'),\n",
    "]\n",
    "\n",
    "for name, yt, yp, color in roc_models:\n",
    "    fpr, tpr, _ = roc_curve(yt, yp)\n",
    "    au = roc_auc_score(yt, yp)\n",
    "    ax.plot(fpr, tpr, lw=2, color=color, label=f'{name}  (AUC={au:.3f})')\n",
    "\n",
    "ax.plot([0,1],[0,1],'k--', lw=1, label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves — All Embedding Methods')\n",
    "ax.legend(loc='lower right', fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-conclusions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify improvements relative to TF-IDF baseline\n",
    "baseline_f1  = df_results.loc['TF-IDF + LR (Balanced+EN)', 'Neg F1']\n",
    "baseline_auc = df_results.loc['TF-IDF + LR (Balanced+EN)', 'AUC-ROC']\n",
    "\n",
    "print('=== Improvement over TF-IDF Baseline ===')\n",
    "for model in df_results.index:\n",
    "    if model == 'TF-IDF + LR (Balanced+EN)':\n",
    "        continue\n",
    "    f1_delta  = df_results.loc[model, 'Neg F1']  - baseline_f1\n",
    "    auc_delta = df_results.loc[model, 'AUC-ROC'] - baseline_auc\n",
    "    print(f'  {model:45s}  ΔNeg F1={f1_delta:+.4f}  ΔAUC={auc_delta:+.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-conclusion-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusions\n",
    "\n",
    "### Expected Findings\n",
    "\n",
    "1. **Word2Vec vs. TF-IDF**: Word2Vec typically provides a modest improvement for tasks with rich semantic variation, but may underperform TF-IDF for classification tasks where lexical features (star-count phrases, domain bigrams) are highly discriminative. The Amazon food review domain has strong lexical signals, which favour TF-IDF.\n",
    "\n",
    "2. **DistilBERT (zero-shot)**: The SST-2 model, trained on movie reviews, may show domain mismatch on food reviews — particularly for domain-specific negative expressions like "returned it" or "taste like cardboard". Expect moderate performance.\n",
    "\n",
    "3. **DistilBERT (fine-tuned)**: After adapting to the Amazon review domain, we expect significant improvements in negative-class recall because the model learns contextual negation patterns ("not what I expected", "won't buy again in context"). **Expected AUC ≥ 0.965**.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Representation | Strengths | Weaknesses |\n",
    "|---|---|---|\n",
    "| TF-IDF | Fast, interpretable, strong lexical features | No semantics, no context, sparse |\n",
    "| Word2Vec | Semantic similarity, dense, generalises | Order-blind, no context, OOV issues |\n",
    "| DistilBERT (zero-shot) | Contextual, no training needed | Domain mismatch |\n",
    "| DistilBERT (fine-tuned) | Best semantics + domain, handles negation | Slow, requires GPU, less interpretable |\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "For production deployment:\n",
    "- **Latency-critical**: TF-IDF + LR (sub-millisecond inference)\n",
    "- **Best quality**: Fine-tuned DistilBERT with threshold tuning\n",
    "- **Middle ground**: Word2Vec + LR (faster inference than BERT, richer features than TF-IDF)\n",
    "\n",
    "### References\n",
    "\n",
    "- Mikolov, T. et al. (2013). *Efficient estimation of word representations in vector space.* ICLR.\n",
    "- Sanh, V. et al. (2019). *DistilBERT, a distilled version of BERT.* NeurIPS Workshop.\n",
    "- Devlin, J. et al. (2019). *BERT: Pre-training of deep bidirectional transformers.* NAACL.\n",
    "- McAuley, J. & Leskovec, J. (2013). *Hidden factors and hidden topics.* RecSys."
   ]
  }
 ]
}
